<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Information Theory | Markov Process: Stationary Distribution</title>
    <meta name="description" content="UvA course">
    <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [ ['$','$'], ['\\(','\\)'] ],processEscapes: true}});</script>
  <script src="/assets/js/MathJax.js?config=TeX-AMS_HTML"></script>
  <script type="application/javascript">function timeout() {setTimeout(function() {MathJax.Hub.Queue(["Typeset", MathJax.Hub]);timeout();}, 1000)};MathJax.Hub.Queue(["Typeset", MathJax.Hub]);timeout();</script>
    
    <link rel="preload" href="/assets/css/styles.c0a0368e.css" as="style"><link rel="preload" href="/assets/js/app.c0a0368e.js" as="script"><link rel="preload" href="/assets/js/57.c78d418a.js" as="script"><link rel="prefetch" href="/assets/js/1.b42459b0.js"><link rel="prefetch" href="/assets/js/10.efcd0718.js"><link rel="prefetch" href="/assets/js/11.b96ece95.js"><link rel="prefetch" href="/assets/js/12.e1fc2cb5.js"><link rel="prefetch" href="/assets/js/13.281a42d7.js"><link rel="prefetch" href="/assets/js/14.321f3d63.js"><link rel="prefetch" href="/assets/js/15.2497a21d.js"><link rel="prefetch" href="/assets/js/16.55966313.js"><link rel="prefetch" href="/assets/js/17.99831763.js"><link rel="prefetch" href="/assets/js/18.b2da39df.js"><link rel="prefetch" href="/assets/js/19.783f6fe5.js"><link rel="prefetch" href="/assets/js/2.4174b9dd.js"><link rel="prefetch" href="/assets/js/20.31a487d3.js"><link rel="prefetch" href="/assets/js/21.a011af4c.js"><link rel="prefetch" href="/assets/js/22.e03bc349.js"><link rel="prefetch" href="/assets/js/23.4463ce34.js"><link rel="prefetch" href="/assets/js/24.b076e7c4.js"><link rel="prefetch" href="/assets/js/25.a123cda5.js"><link rel="prefetch" href="/assets/js/26.c5548a4d.js"><link rel="prefetch" href="/assets/js/27.91697a5a.js"><link rel="prefetch" href="/assets/js/28.26582aab.js"><link rel="prefetch" href="/assets/js/29.cc517640.js"><link rel="prefetch" href="/assets/js/3.4c5adbde.js"><link rel="prefetch" href="/assets/js/30.3ca179bc.js"><link rel="prefetch" href="/assets/js/31.3e961e9b.js"><link rel="prefetch" href="/assets/js/32.eb1a29f5.js"><link rel="prefetch" href="/assets/js/33.14f4f43a.js"><link rel="prefetch" href="/assets/js/34.af720415.js"><link rel="prefetch" href="/assets/js/35.d285264b.js"><link rel="prefetch" href="/assets/js/36.44d3152b.js"><link rel="prefetch" href="/assets/js/37.b076d761.js"><link rel="prefetch" href="/assets/js/38.ad1f27c7.js"><link rel="prefetch" href="/assets/js/39.413161ea.js"><link rel="prefetch" href="/assets/js/4.15836b88.js"><link rel="prefetch" href="/assets/js/40.ba4bef5e.js"><link rel="prefetch" href="/assets/js/41.6fbb1017.js"><link rel="prefetch" href="/assets/js/42.80c5b28a.js"><link rel="prefetch" href="/assets/js/43.3f0aeafe.js"><link rel="prefetch" href="/assets/js/44.c08a8e84.js"><link rel="prefetch" href="/assets/js/45.2da068e9.js"><link rel="prefetch" href="/assets/js/46.cf98a01d.js"><link rel="prefetch" href="/assets/js/47.95ad89e2.js"><link rel="prefetch" href="/assets/js/48.11de7b32.js"><link rel="prefetch" href="/assets/js/49.89191d18.js"><link rel="prefetch" href="/assets/js/5.64f16959.js"><link rel="prefetch" href="/assets/js/50.d3c00218.js"><link rel="prefetch" href="/assets/js/51.270d7491.js"><link rel="prefetch" href="/assets/js/52.e303f682.js"><link rel="prefetch" href="/assets/js/53.6b37d452.js"><link rel="prefetch" href="/assets/js/54.5516a56a.js"><link rel="prefetch" href="/assets/js/55.1535f45c.js"><link rel="prefetch" href="/assets/js/56.fe5d6cd5.js"><link rel="prefetch" href="/assets/js/58.6a5328b4.js"><link rel="prefetch" href="/assets/js/59.9c425111.js"><link rel="prefetch" href="/assets/js/6.56b74cc5.js"><link rel="prefetch" href="/assets/js/60.8471b9ec.js"><link rel="prefetch" href="/assets/js/61.7f1373c2.js"><link rel="prefetch" href="/assets/js/62.3fa31180.js"><link rel="prefetch" href="/assets/js/63.9778d19f.js"><link rel="prefetch" href="/assets/js/64.7ec547c6.js"><link rel="prefetch" href="/assets/js/65.46b6abc2.js"><link rel="prefetch" href="/assets/js/66.41e96d6e.js"><link rel="prefetch" href="/assets/js/67.1e438865.js"><link rel="prefetch" href="/assets/js/68.ec61f597.js"><link rel="prefetch" href="/assets/js/69.a4ad7ed9.js"><link rel="prefetch" href="/assets/js/7.6e55be9c.js"><link rel="prefetch" href="/assets/js/70.ad9257f5.js"><link rel="prefetch" href="/assets/js/71.e45ee61f.js"><link rel="prefetch" href="/assets/js/72.cf56042d.js"><link rel="prefetch" href="/assets/js/73.5190c50f.js"><link rel="prefetch" href="/assets/js/74.a5427366.js"><link rel="prefetch" href="/assets/js/75.ea188344.js"><link rel="prefetch" href="/assets/js/76.69afb87b.js"><link rel="prefetch" href="/assets/js/77.91aca632.js"><link rel="prefetch" href="/assets/js/78.e5c052d0.js"><link rel="prefetch" href="/assets/js/79.a790ac95.js"><link rel="prefetch" href="/assets/js/8.ef38795e.js"><link rel="prefetch" href="/assets/js/80.65aec641.js"><link rel="prefetch" href="/assets/js/81.78289e97.js"><link rel="prefetch" href="/assets/js/82.08de41e8.js"><link rel="prefetch" href="/assets/js/83.ff9c1755.js"><link rel="prefetch" href="/assets/js/84.f86545fe.js"><link rel="prefetch" href="/assets/js/85.c883b25b.js"><link rel="prefetch" href="/assets/js/86.1d29daeb.js"><link rel="prefetch" href="/assets/js/87.9c24da6f.js"><link rel="prefetch" href="/assets/js/88.bcbf388d.js"><link rel="prefetch" href="/assets/js/89.e3aaff55.js"><link rel="prefetch" href="/assets/js/9.3f4a2127.js"><link rel="prefetch" href="/assets/js/90.23eda780.js"><link rel="prefetch" href="/assets/js/91.da84bead.js"><link rel="prefetch" href="/assets/js/92.7d2a6af1.js"><link rel="prefetch" href="/assets/js/93.4de0373f.js">
    <link rel="stylesheet" href="/assets/css/styles.c0a0368e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div><a href="/" class="home-link router-link-active"><!----><span class="site-name">
      Information Theory
    </span></a><div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""><!----></div><!----></div></header><div class="sidebar-mask"></div><div class="sidebar"><!----><ul class="sidebar-links"><li><div class="sidebar-group first collapsable"><p class="sidebar-heading"><span>general information</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>01 probability theory</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>02 entropy</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>03 source coding data compression</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>04 typical sets and encryption</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading open"><span>05 random processes</span><span class="arrow down"></span></p><ul class="sidebar-group-items"><li><a href="/05-random-processes/markov-chains/markov-chains-of-length-3/" class="sidebar-link">Markov Chains of Length 3</a></li><li><a href="/05-random-processes/markov-chains/data-processing-inequality/" class="sidebar-link">Data-Processing Inequality</a></li><li><a href="/05-random-processes/markov-chains/sufficient-statistics/" class="sidebar-link">Sufficient Statistics</a></li><li><a href="/05-random-processes/markov-chains/definition-longer-markov-chains/" class="sidebar-link">Definition: Longer Markov Chains</a></li><li><a href="/05-random-processes/stochastic-processes/discrete-time-stochastic-process/" class="sidebar-link">Discrete-Time Stochastic Process</a></li><li><a href="/05-random-processes/stochastic-processes/stationary-process/" class="sidebar-link">Stationary Process</a></li><li><a href="/05-random-processes/stochastic-processes/markov-process-time-invariance-finite-state-transition-matrix/" class="sidebar-link">Markov Process: Time Invariance, Finite State, Transition Matrix</a></li><li><a href="/05-random-processes/stochastic-processes/markov-process-stationary-distribution/" class="active sidebar-link">Markov Process: Stationary Distribution</a></li><li><a href="/05-random-processes/stochastic-processes/markov-process-irreducibility-periodicity-convergence/" class="sidebar-link">Markov Process: Irreducibility, Periodicity, Convergence</a></li><li><a href="/05-random-processes/entropy-rate/entropy-rate/" class="sidebar-link">Entropy Rate</a></li><li><a href="/05-random-processes/entropy-rate/aep-for-ergodic-stationary-processes/" class="sidebar-link">AEP for Ergodic Stationary Processes</a></li><li><a href="/05-random-processes/more-markov-processes/random-walks-on-graphs/" class="sidebar-link">Random Walks on Graphs</a></li></ul></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>06 error correction and zero error transmission</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>07 noisy channel coding</span><span class="arrow right"></span></p><!----></div></li></ul></div><div class="page"><div class="content"><h1 id="markov-process-stationary-distribution"><a href="#markov-process-stationary-distribution" aria-hidden="true" class="header-anchor">#</a> Markov Process: Stationary Distribution</h1><p><img src="https://canvas.uva.nl/courses/2205/files/413835/preview?verifier=d8BwJdQOyZ4E7qED9VJvB2wvo3vjyweCdjfteTkB" alt="RandomProcess.png" width="600" height="488" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/413835" data-api-returntype="File"></p><p>Suppose that we run the process of <a href="https://canvas.uva.nl/courses/2205/pages/markov-process-time-invariance-finite-state-transition-matrix#example2" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/markov-process-time-invariance-finite-state-transition-matrix%23example2" data-api-returntype="Page">Example 2</a> for a very large number of steps, and wonder what the probability will be of observing an \(\texttt{a}\) at the next step. Given the initial distribution and the state diagram, we can compute the probability distribution for every \(X_i\). In the figure above, \(P_{X_i}(\texttt{a})\) is plotted for several values of \(i\). The probability to observe an \(\texttt{a}\) seems to stabilize. This leads us to the following definition:</p><div class="content-box pad-box-mini border border-trbl border-round"><h4 style="color: #bc0031;"><strong>Definition: Stationary distribution</strong></h4>
A stationary distribution for a time-invariant Markov chain is a distribution \(P_{X_n}\) such that \(P_{X_{n+1}} = P_{X_n}\).</div><p>If the initial distribution of a time-invariant Markov process is stationary, then the entire process is stationary as <a title="Stationary Process" href="https://canvas.uva.nl/courses/2205/pages/stationary-process" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/stationary-process" data-api-returntype="Page">defined previously</a>.</p><div class="content-box pad-box-mini border border-trbl border-round"><h4 style="color: #bc0031;"><strong>Proposition</strong></h4>
Every time-invariant finite-state Markov process has a stationary distribution.
<p><span role="button" aria-controls="group12" aria-label="Toggler" aria-expanded="false" class="element_toggler"><span class="Button">Proof</span></span></p><div id="group12"><div class="content-box">Let \(k:= |\mathcal{X}|\). The \(k \times k\) transition matrix \(R\) with entries \(R_{ij} = P_{X_{n+1}|X_n}(j|i)\) is a <a href="https://en.wikipedia.org/wiki/Stochastic_matrix">stochastic matrix</a>, as for every row \(i\), the sum over columns is \(\sum_{j=1}^k R_{ij} = 1\). We are interested in finding a vector \(v \in \mathbb{R}_{\geq 0}^k\) such that \(\|v\| = 1\) and \(R^Tv = v\). This vector then represents the stationary distribution. Clearly, a possible eigenvector for \(R\) is the all-1 vector \(w=(1,\ldots,1)^T\) because \(R w = w\) by definition of a stochastic matrix. Hence, 1 is an eigenvalue of \(R\). As \(R\) and \(R^T\) <a href="https://math.stackexchange.com/questions/123923/a-matrix-and-its-transpose-have-the-same-set-of-eigenvalues/123927">have the same eigenvalues</a>, 1 is also an eigenvalue of \(R^T\); let \(v \in \mathbb{R}^k\) be the corresponding eigenvector such that \(R^T v = v\). If all coordinates of \(v\) are non-negative, one can verify that we have found a stationary distribution by renormalizing \(v / \sum_{i=1}^k v_i\). Otherwise, let us write \(v = v^+ - v^-\) with \(v^+,v^- \in \mathbb{R}_{\geq 0}^k\), where we put all positive coordinates of \( v \) in \(v^+\) and all negative coordinates of \( v \) in \(v^-\).  Note that \(R^T v^+ - R^T v^- = R^T (v^+ - v^-) = R^T v = v = v^+ - v^-\). As all entries of \(R^T, v^+\) and \(v^-\) are positive, equality must hold for both the positive and negative parts: \(R^T v^+ = v^+\) and \(R^T v^- = v^-\). As either \(v^+ \neq 0^k\) or \(v^- \neq 0^k\) (otherwise \(v = 0^k\), which cannot be the case for an eigenvector), renormalizing that non-zero vector as above yields the stationary distribution.</div></div></div><p>Given the transition matrix \(R\) of a finite-state Markov process, one can find the stationary distribution \( \mu \) by solving the linear equation \( \mu R = \mu \) under the constraint that \( \sum_i \mu_i = 1\).</p><div class="content-box pad-box-mini border border-trbl border-round"><h4 id="example2" style="color: #2d3b45;"><strong>Example 2: A finite-state time-invariant Markov process, continued</strong></h4>
The matrix representation of the process above is given by \[ R = \left[ \begin{array}{c c} 0.7&amp;0.3\\ 0.5&amp;0.5 \end{array} \right] \] . Writing out \( (\mu_a, \mu_b) R = (\mu_a, \mu_b) \) results in \[ \left\{ \begin{array}{l} 0.7 \mu_a + 0.5 \mu_b = \mu_a \\ 0.3 \mu_a + 0.5 \mu_b = \mu_b \end{array} \right. \]. These are linearly dependent equations, but together with the constraint \( \mu_a + \mu_b = 1 \), they can be solved to \( (\mu_a , \mu_b) = (5/8, 3/8) \).</div></div><!----><div class="content page-nav"><p class="inner"><span class="prev">
        ← <a href="/05-random-processes/stochastic-processes/markov-process-time-invariance-finite-state-transition-matrix/" class="prev">
          Markov Process: Time Invariance, Finite State, Transition Matrix
        </a></span><span class="next"><a href="/05-random-processes/stochastic-processes/markov-process-irreducibility-periodicity-convergence/">
          Markov Process: Irreducibility, Periodicity, Convergence
        </a> →
      </span></p></div></div></div></div>
    <script src="/assets/js/57.c78d418a.js" defer></script><script src="/assets/js/app.c0a0368e.js" defer></script>
  </body>
</html>
