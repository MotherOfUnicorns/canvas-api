<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Information Theory | Definition: Channel Capacity</title>
    <meta name="description" content="UvA course">
    <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [ ['$','$'], ['\\(','\\)'] ],processEscapes: true}});</script>
  <script src="/assets/js/MathJax.js?config=TeX-AMS_HTML"></script>
  <script type="application/javascript">function timeout() {setTimeout(function() {MathJax.Hub.Queue(["Typeset", MathJax.Hub]);timeout();}, 1000)};MathJax.Hub.Queue(["Typeset", MathJax.Hub]);timeout();</script>
    
    <link rel="preload" href="/assets/css/styles.c0a0368e.css" as="style"><link rel="preload" href="/assets/js/app.c0a0368e.js" as="script"><link rel="preload" href="/assets/js/60.8471b9ec.js" as="script"><link rel="prefetch" href="/assets/js/1.b42459b0.js"><link rel="prefetch" href="/assets/js/10.efcd0718.js"><link rel="prefetch" href="/assets/js/11.b96ece95.js"><link rel="prefetch" href="/assets/js/12.e1fc2cb5.js"><link rel="prefetch" href="/assets/js/13.281a42d7.js"><link rel="prefetch" href="/assets/js/14.321f3d63.js"><link rel="prefetch" href="/assets/js/15.2497a21d.js"><link rel="prefetch" href="/assets/js/16.55966313.js"><link rel="prefetch" href="/assets/js/17.99831763.js"><link rel="prefetch" href="/assets/js/18.b2da39df.js"><link rel="prefetch" href="/assets/js/19.783f6fe5.js"><link rel="prefetch" href="/assets/js/2.4174b9dd.js"><link rel="prefetch" href="/assets/js/20.31a487d3.js"><link rel="prefetch" href="/assets/js/21.a011af4c.js"><link rel="prefetch" href="/assets/js/22.e03bc349.js"><link rel="prefetch" href="/assets/js/23.4463ce34.js"><link rel="prefetch" href="/assets/js/24.b076e7c4.js"><link rel="prefetch" href="/assets/js/25.a123cda5.js"><link rel="prefetch" href="/assets/js/26.c5548a4d.js"><link rel="prefetch" href="/assets/js/27.91697a5a.js"><link rel="prefetch" href="/assets/js/28.26582aab.js"><link rel="prefetch" href="/assets/js/29.cc517640.js"><link rel="prefetch" href="/assets/js/3.4c5adbde.js"><link rel="prefetch" href="/assets/js/30.3ca179bc.js"><link rel="prefetch" href="/assets/js/31.3e961e9b.js"><link rel="prefetch" href="/assets/js/32.eb1a29f5.js"><link rel="prefetch" href="/assets/js/33.14f4f43a.js"><link rel="prefetch" href="/assets/js/34.af720415.js"><link rel="prefetch" href="/assets/js/35.d285264b.js"><link rel="prefetch" href="/assets/js/36.44d3152b.js"><link rel="prefetch" href="/assets/js/37.b076d761.js"><link rel="prefetch" href="/assets/js/38.ad1f27c7.js"><link rel="prefetch" href="/assets/js/39.413161ea.js"><link rel="prefetch" href="/assets/js/4.15836b88.js"><link rel="prefetch" href="/assets/js/40.ba4bef5e.js"><link rel="prefetch" href="/assets/js/41.6fbb1017.js"><link rel="prefetch" href="/assets/js/42.80c5b28a.js"><link rel="prefetch" href="/assets/js/43.3f0aeafe.js"><link rel="prefetch" href="/assets/js/44.c08a8e84.js"><link rel="prefetch" href="/assets/js/45.2da068e9.js"><link rel="prefetch" href="/assets/js/46.cf98a01d.js"><link rel="prefetch" href="/assets/js/47.95ad89e2.js"><link rel="prefetch" href="/assets/js/48.11de7b32.js"><link rel="prefetch" href="/assets/js/49.89191d18.js"><link rel="prefetch" href="/assets/js/5.64f16959.js"><link rel="prefetch" href="/assets/js/50.d3c00218.js"><link rel="prefetch" href="/assets/js/51.270d7491.js"><link rel="prefetch" href="/assets/js/52.e303f682.js"><link rel="prefetch" href="/assets/js/53.6b37d452.js"><link rel="prefetch" href="/assets/js/54.5516a56a.js"><link rel="prefetch" href="/assets/js/55.1535f45c.js"><link rel="prefetch" href="/assets/js/56.fe5d6cd5.js"><link rel="prefetch" href="/assets/js/57.c78d418a.js"><link rel="prefetch" href="/assets/js/58.6a5328b4.js"><link rel="prefetch" href="/assets/js/59.9c425111.js"><link rel="prefetch" href="/assets/js/6.56b74cc5.js"><link rel="prefetch" href="/assets/js/61.7f1373c2.js"><link rel="prefetch" href="/assets/js/62.3fa31180.js"><link rel="prefetch" href="/assets/js/63.9778d19f.js"><link rel="prefetch" href="/assets/js/64.7ec547c6.js"><link rel="prefetch" href="/assets/js/65.46b6abc2.js"><link rel="prefetch" href="/assets/js/66.41e96d6e.js"><link rel="prefetch" href="/assets/js/67.1e438865.js"><link rel="prefetch" href="/assets/js/68.ec61f597.js"><link rel="prefetch" href="/assets/js/69.a4ad7ed9.js"><link rel="prefetch" href="/assets/js/7.6e55be9c.js"><link rel="prefetch" href="/assets/js/70.ad9257f5.js"><link rel="prefetch" href="/assets/js/71.e45ee61f.js"><link rel="prefetch" href="/assets/js/72.cf56042d.js"><link rel="prefetch" href="/assets/js/73.5190c50f.js"><link rel="prefetch" href="/assets/js/74.a5427366.js"><link rel="prefetch" href="/assets/js/75.ea188344.js"><link rel="prefetch" href="/assets/js/76.69afb87b.js"><link rel="prefetch" href="/assets/js/77.91aca632.js"><link rel="prefetch" href="/assets/js/78.e5c052d0.js"><link rel="prefetch" href="/assets/js/79.a790ac95.js"><link rel="prefetch" href="/assets/js/8.ef38795e.js"><link rel="prefetch" href="/assets/js/80.65aec641.js"><link rel="prefetch" href="/assets/js/81.78289e97.js"><link rel="prefetch" href="/assets/js/82.08de41e8.js"><link rel="prefetch" href="/assets/js/83.ff9c1755.js"><link rel="prefetch" href="/assets/js/84.f86545fe.js"><link rel="prefetch" href="/assets/js/85.c883b25b.js"><link rel="prefetch" href="/assets/js/86.1d29daeb.js"><link rel="prefetch" href="/assets/js/87.9c24da6f.js"><link rel="prefetch" href="/assets/js/88.bcbf388d.js"><link rel="prefetch" href="/assets/js/89.e3aaff55.js"><link rel="prefetch" href="/assets/js/9.3f4a2127.js"><link rel="prefetch" href="/assets/js/90.23eda780.js"><link rel="prefetch" href="/assets/js/91.da84bead.js"><link rel="prefetch" href="/assets/js/92.7d2a6af1.js"><link rel="prefetch" href="/assets/js/93.4de0373f.js">
    <link rel="stylesheet" href="/assets/css/styles.c0a0368e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div><a href="/" class="home-link router-link-active"><!----><span class="site-name">
      Information Theory
    </span></a><div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""><!----></div><!----></div></header><div class="sidebar-mask"></div><div class="sidebar"><!----><ul class="sidebar-links"><li><div class="sidebar-group first collapsable"><p class="sidebar-heading"><span>general information</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>01 probability theory</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>02 entropy</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>03 source coding data compression</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>04 typical sets and encryption</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>05 random processes</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading open"><span>06 error correction and zero error transmission</span><span class="arrow down"></span></p><ul class="sidebar-group-items"><li><a href="/06-error-correction-and-zero-error-transmission/noisy-channels/introduction-noise/" class="sidebar-link">Introduction: Noise</a></li><li><a href="/06-error-correction-and-zero-error-transmission/noisy-channels/definition-discrete-channel/" class="sidebar-link">Definition: Discrete Channel</a></li><li><a href="/06-error-correction-and-zero-error-transmission/noisy-channels/types-of-discrete-channels/" class="sidebar-link">Types of Discrete Channels</a></li><li><a href="/06-error-correction-and-zero-error-transmission/error-correcting-codes/definitions-code-rate-and-error-probability/" class="sidebar-link">Definitions: Code, Rate, and Error Probability</a></li><li><a href="/06-error-correction-and-zero-error-transmission/error-correcting-codes/the-repetition-code/" class="sidebar-link">The Repetition Code</a></li><li><a href="/06-error-correction-and-zero-error-transmission/error-correcting-codes/the-74-hamming-code/" class="sidebar-link">The [7,4] Hamming Code</a></li><li><a href="/06-error-correction-and-zero-error-transmission/error-correcting-codes/minimal-distance-between-codewords/" class="sidebar-link">Minimal Distance Between Codewords</a></li><li><a href="/06-error-correction-and-zero-error-transmission/error-correcting-codes/error-detectioncorrection-and-the-hamming-bound/" class="sidebar-link">Error Detection/Correction and the Hamming Bound</a></li><li><a href="/06-error-correction-and-zero-error-transmission/linear-error-correcting-codes/definition-linear-code/" class="sidebar-link">Definition: Linear Code</a></li><li><a href="/06-error-correction-and-zero-error-transmission/linear-error-correcting-codes/encoding-generator-matrix/" class="sidebar-link">Encoding: Generator Matrix</a></li><li><a href="/06-error-correction-and-zero-error-transmission/linear-error-correcting-codes/decoding-parity-check-matrix-and-error-syndromes/" class="sidebar-link">Decoding: Parity-Check Matrix and Error Syndromes</a></li><li><a href="/06-error-correction-and-zero-error-transmission/linear-error-correcting-codes/minimal-distance-of-linear-codes/" class="sidebar-link">Minimal Distance of Linear Codes</a></li><li><a href="/06-error-correction-and-zero-error-transmission/zero-error-channel-coding/introduction-zero-error-channel-coding/" class="sidebar-link">Introduction: Zero-Error Channel Coding</a></li><li><a href="/06-error-correction-and-zero-error-transmission/zero-error-channel-coding/definition-independence-number/" class="sidebar-link">Definition: Independence Number</a></li><li><a href="/06-error-correction-and-zero-error-transmission/zero-error-channel-coding/definition-confusability-graph/" class="sidebar-link">Definition: Confusability Graph</a></li><li><a href="/06-error-correction-and-zero-error-transmission/zero-error-channel-coding/definition-strong-graph-product/" class="sidebar-link">Definition: Strong Graph Product</a></li><li><a href="/06-error-correction-and-zero-error-transmission/zero-error-channel-coding/multiple-channel-uses/" class="sidebar-link">Multiple Channel Uses</a></li><li><a href="/06-error-correction-and-zero-error-transmission/zero-error-channel-coding/shannon-capacity-of-a-graph/" class="sidebar-link">Shannon Capacity of a Graph</a></li><li><a href="/06-error-correction-and-zero-error-transmission/channel-capacity/definition-channel-capacity/" class="active sidebar-link">Definition: Channel Capacity</a></li></ul></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>07 noisy channel coding</span><span class="arrow right"></span></p><!----></div></li></ul></div><div class="page"><div class="content"><h1 id="definition-channel-capacity"><a href="#definition-channel-capacity" aria-hidden="true" class="header-anchor">#</a> Definition: Channel Capacity</h1><p>We just discovered that for some noisy channels, zero-error communication is very hard, or even impossible. For example, if Alice and Bob have to communicate over a <a title="Definition: Discrete Channel" href="https://canvas.uva.nl/courses/2205/pages/definition-discrete-channel" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/definition-discrete-channel" data-api-returntype="Page">binary symmetric channel (BSC)</a> that has non-zero bit-flip probability, they cannot hope to do any zero-error communication, because the Shannon capacity of the BSC's confusability graph is zero.</p><p>We also saw that error-correcting codes can help deal with such inherently noisy channels. Even though the communication error may not become zero, an error-correcting code can increase the probability of receiving the correct message. It does come at a cost, however, because the codewords are longer than the original messages, and so the amount of information that is transmitted <i>per channel use</i> does not necessarily increase.</p><p>In this final part of the module, we explore the limits of how much information can be sent over a channel if a small error is allowed. Central to our study will be the concept of channel capacity. It reflects the maximum amount of information that could <i>in principle</i> be communicated with a single use of a channel. In the next module, we will see how well that theoretical limit can be approached with actual error-correcting codes.</p><div class="content-box pad-box-mini border border-trbl border-round"><h4 style="color: #bc0031;"><strong>Definition: Channel capacity</strong></h4>
The channel capacity \(C\) of a discrete, memoryless channel \((\mathcal{X}, P_{Y|X}, \mathcal{Y})\) is given by \[ C:= \max_{P_X} I(X;Y). \]</div><p>Remember that using a certain input distribution \(P_X \) for a channel \( P_{Y|X} \) yields a joint input-output distribution \(P_{XY}\) which determines the real quantity \(I(X;Y) \) we can optimize over. One can <a title="The set of joint distributions { P_{XY} } is compact, and the mutual information is a continuous function from that set to the real numbers. It follows from the extreme-value theorem that the maximum is attained. " data-tooltip="{"tooltipClass":"popover popover-padded", "position":"right"}">argue</a> that the maximum is attained and therefore the channel capacity is a well-defined quantity.</p><p>Important: the channel capacity is often called the Shannon capacity (of a channel). You should not confuse it with the <a title="Shannon Capacity of a Graph" href="https://canvas.uva.nl/courses/2205/pages/shannon-capacity-of-a-graph" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/shannon-capacity-of-a-graph" data-api-returntype="Page">Shannon Capacity of a Graph</a>. Generally, the Shannon capacity of a channel is not equal to the Shannon capacity of its confusability graph.</p><div class="content-box pad-box-mini border border-trbl border-round"><h4 style="color: #2d3b45;"><strong>Example: Capacity of a BSC</strong></h4>
What is the capacity (in terms of \(f\)) of a binary symmetric channel with parameter \(f \in [0,1/2]\)?
<p><span role="button" aria-controls="group1a" aria-label="Toggler" aria-expanded="false" class="element_toggler"><span class="Button">Show hint</span></span></p><div id="group1a"><div class="content-box">Rewrite \(I(X;Y)\) as \(H(Y) - H(Y|X)\) and note that you can compute \(H(Y|X)\) without fixing \(P_X\). Then think about how to maximize \(H(Y)\).
<p><span role="button" aria-controls="group1b" aria-label="Toggler" aria-expanded="false" class="element_toggler"><span class="Button">Show solution</span></span></p><div id="group1b"><div class="content-box">The channel capacity is \begin{align} \max_{P_X} I(X;Y) &amp;= \max_{P_X} \left( H(Y) - H(Y|X)\right)\\ &amp;= \max_{P_X} \left( H(Y) - \sum_{x \in \mathcal{X}} P_X(x) \cdot H(Y|X=x)\right) \\ &amp;= \max_{P_X} \left( H(Y) - \sum_{x \in \mathcal{X}} P_X(X) \cdot h(f)\right) \\ &amp;= \max_{P_X} \left( H(Y) - h(f)\right) \\ &amp;= 1- h(f). \end{align} The last step follows because \(H(Y)\) is maximized if \(Y\) is uniform, which is achievable by choosing \(X\) to be uniform.</div></div></div></div></div><div class="content-box pad-box-mini border border-trbl border-round"><h4 style="color: #2d3b45;"><strong>Example: Capacity of a BEC</strong></h4><p>Consider the binary erasure channel (BEC) with \(\mathcal{X} = \{0,1\}\) and \(\mathcal{Y} = \{0,1,\bot\}\), where \(\bot\) is the <span style="color: #bc0031;"><strong>erasure symbol</strong></span>, and \(\epsilon \in [0,1]\) is the <span style="color: #bc0031;"><strong>erasure probability</strong></span>:</p><p style="text-align: center;"><img src="https://canvas.uva.nl/courses/2205/files/388658/preview?verifier=E9ranqn35eYPibIXdJ5XcEG1CWqNXu5KVumwLJUd" alt="A binary erasure channel" width="240" height="140" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/388658" data-api-returntype="File"></p><p>What is the channel capacity of the BEC, as a function of \(\epsilon\)?</p><p><span role="button" aria-controls="group2a" aria-label="Toggler" aria-expanded="false" class="element_toggler"><span class="Button">Show hint</span></span></p><div id="group2a"><div class="content-box">Contrary to the previous example, break \(I(X;Y)\) up as \(H(X) - H(X|Y)\), using symmetry of the mutual information. Consider the three possible outputs separately: how much uncertainty is left if you receive output 0? What about output 1? And output \(\bot\)?
<p><span role="button" aria-controls="group2b" aria-label="Toggler" aria-expanded="false" class="element_toggler"><span class="Button">Show solution</span></span></p><div id="group2b"><div class="content-box">Write \(p\) for \(P_X(0)\). \begin{align*} \max_{P_X} I(X;Y) &amp;= \max_{P_X} \left( H(X) - H(X|Y)\right) \\ &amp;= \max_{p} \left( h(p) - \sum_{y \in \mathcal{Y}} P_Y(y) \cdot H(X|Y=y)\right) \\ &amp;= \max_{p} \left( h(p) - P_Y(\bot) \cdot h(p)\right) \\ &amp;= \max_{p} \left( h(p) (1-\epsilon)\right) \\ &amp;= 1 - \epsilon \, . \end{align*} Again, the last step follows because \(H(X)=h(p)\) is maximized if \(X\) is uniform, hence \(p=\frac12\).</div></div></div></div></div><p>If a channel is memoryless, then using it more than once does not increase the capacity <i>per transmission</i>. Note that this is different from the zero-error setting, where multiple channel uses can in fact increase the efficiency of getting information across! This is formally captured in the following lemma, which we state without proof:</p><div class="content-box pad-box-mini border border-trbl border-round"><h4 id="lemma" style="color: #bc0031;"><strong>Lemma: Multiple Channel Uses</strong></h4>
Let \(X_1, ..., X_n =: X^n\) be \(n\) random variables. Let \(Y^n\) be the result of passing \(X^n\) through a discrete memoryless channel of capacity \(C\). Then for any joint distribution \(P_{X^n}\), \[ I(X^n,Y^n) \leq n \cdot C. \]</div></div><!----><div class="content page-nav"><p class="inner"><span class="prev">
        ← <a href="/06-error-correction-and-zero-error-transmission/zero-error-channel-coding/shannon-capacity-of-a-graph/" class="prev">
          Shannon Capacity of a Graph
        </a></span><span class="next"><a href="/07-noisy-channel-coding/definition-achievable-rate/">
          Definition: Achievable Rate
        </a> →
      </span></p></div></div></div></div>
    <script src="/assets/js/60.8471b9ec.js" defer></script><script src="/assets/js/app.c0a0368e.js" defer></script>
  </body>
</html>
