<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Information Theory | Noisy-Channel Theorem: Forward Direction</title>
    <meta name="description" content="UvA course">
    <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [ ['$','$'], ['\\(','\\)'] ],processEscapes: true}});</script>
  <script src="/assets/js/MathJax.js?config=TeX-AMS_HTML"></script>
  <script type="application/javascript">function timeout() {setTimeout(function() {MathJax.Hub.Queue(["Typeset", MathJax.Hub]);timeout();}, 1000)};MathJax.Hub.Queue(["Typeset", MathJax.Hub]);timeout();</script>
    
    <link rel="preload" href="/assets/css/styles.c0a0368e.css" as="style"><link rel="preload" href="/assets/js/app.c0a0368e.js" as="script"><link rel="preload" href="/assets/js/84.f86545fe.js" as="script"><link rel="prefetch" href="/assets/js/1.b42459b0.js"><link rel="prefetch" href="/assets/js/10.efcd0718.js"><link rel="prefetch" href="/assets/js/11.b96ece95.js"><link rel="prefetch" href="/assets/js/12.e1fc2cb5.js"><link rel="prefetch" href="/assets/js/13.281a42d7.js"><link rel="prefetch" href="/assets/js/14.321f3d63.js"><link rel="prefetch" href="/assets/js/15.2497a21d.js"><link rel="prefetch" href="/assets/js/16.55966313.js"><link rel="prefetch" href="/assets/js/17.99831763.js"><link rel="prefetch" href="/assets/js/18.b2da39df.js"><link rel="prefetch" href="/assets/js/19.783f6fe5.js"><link rel="prefetch" href="/assets/js/2.4174b9dd.js"><link rel="prefetch" href="/assets/js/20.31a487d3.js"><link rel="prefetch" href="/assets/js/21.a011af4c.js"><link rel="prefetch" href="/assets/js/22.e03bc349.js"><link rel="prefetch" href="/assets/js/23.4463ce34.js"><link rel="prefetch" href="/assets/js/24.b076e7c4.js"><link rel="prefetch" href="/assets/js/25.a123cda5.js"><link rel="prefetch" href="/assets/js/26.c5548a4d.js"><link rel="prefetch" href="/assets/js/27.91697a5a.js"><link rel="prefetch" href="/assets/js/28.26582aab.js"><link rel="prefetch" href="/assets/js/29.cc517640.js"><link rel="prefetch" href="/assets/js/3.4c5adbde.js"><link rel="prefetch" href="/assets/js/30.3ca179bc.js"><link rel="prefetch" href="/assets/js/31.3e961e9b.js"><link rel="prefetch" href="/assets/js/32.eb1a29f5.js"><link rel="prefetch" href="/assets/js/33.14f4f43a.js"><link rel="prefetch" href="/assets/js/34.af720415.js"><link rel="prefetch" href="/assets/js/35.d285264b.js"><link rel="prefetch" href="/assets/js/36.44d3152b.js"><link rel="prefetch" href="/assets/js/37.b076d761.js"><link rel="prefetch" href="/assets/js/38.ad1f27c7.js"><link rel="prefetch" href="/assets/js/39.413161ea.js"><link rel="prefetch" href="/assets/js/4.15836b88.js"><link rel="prefetch" href="/assets/js/40.ba4bef5e.js"><link rel="prefetch" href="/assets/js/41.6fbb1017.js"><link rel="prefetch" href="/assets/js/42.80c5b28a.js"><link rel="prefetch" href="/assets/js/43.3f0aeafe.js"><link rel="prefetch" href="/assets/js/44.c08a8e84.js"><link rel="prefetch" href="/assets/js/45.2da068e9.js"><link rel="prefetch" href="/assets/js/46.cf98a01d.js"><link rel="prefetch" href="/assets/js/47.95ad89e2.js"><link rel="prefetch" href="/assets/js/48.11de7b32.js"><link rel="prefetch" href="/assets/js/49.89191d18.js"><link rel="prefetch" href="/assets/js/5.64f16959.js"><link rel="prefetch" href="/assets/js/50.d3c00218.js"><link rel="prefetch" href="/assets/js/51.270d7491.js"><link rel="prefetch" href="/assets/js/52.e303f682.js"><link rel="prefetch" href="/assets/js/53.6b37d452.js"><link rel="prefetch" href="/assets/js/54.5516a56a.js"><link rel="prefetch" href="/assets/js/55.1535f45c.js"><link rel="prefetch" href="/assets/js/56.fe5d6cd5.js"><link rel="prefetch" href="/assets/js/57.c78d418a.js"><link rel="prefetch" href="/assets/js/58.6a5328b4.js"><link rel="prefetch" href="/assets/js/59.9c425111.js"><link rel="prefetch" href="/assets/js/6.56b74cc5.js"><link rel="prefetch" href="/assets/js/60.8471b9ec.js"><link rel="prefetch" href="/assets/js/61.7f1373c2.js"><link rel="prefetch" href="/assets/js/62.3fa31180.js"><link rel="prefetch" href="/assets/js/63.9778d19f.js"><link rel="prefetch" href="/assets/js/64.7ec547c6.js"><link rel="prefetch" href="/assets/js/65.46b6abc2.js"><link rel="prefetch" href="/assets/js/66.41e96d6e.js"><link rel="prefetch" href="/assets/js/67.1e438865.js"><link rel="prefetch" href="/assets/js/68.ec61f597.js"><link rel="prefetch" href="/assets/js/69.a4ad7ed9.js"><link rel="prefetch" href="/assets/js/7.6e55be9c.js"><link rel="prefetch" href="/assets/js/70.ad9257f5.js"><link rel="prefetch" href="/assets/js/71.e45ee61f.js"><link rel="prefetch" href="/assets/js/72.cf56042d.js"><link rel="prefetch" href="/assets/js/73.5190c50f.js"><link rel="prefetch" href="/assets/js/74.a5427366.js"><link rel="prefetch" href="/assets/js/75.ea188344.js"><link rel="prefetch" href="/assets/js/76.69afb87b.js"><link rel="prefetch" href="/assets/js/77.91aca632.js"><link rel="prefetch" href="/assets/js/78.e5c052d0.js"><link rel="prefetch" href="/assets/js/79.a790ac95.js"><link rel="prefetch" href="/assets/js/8.ef38795e.js"><link rel="prefetch" href="/assets/js/80.65aec641.js"><link rel="prefetch" href="/assets/js/81.78289e97.js"><link rel="prefetch" href="/assets/js/82.08de41e8.js"><link rel="prefetch" href="/assets/js/83.ff9c1755.js"><link rel="prefetch" href="/assets/js/85.c883b25b.js"><link rel="prefetch" href="/assets/js/86.1d29daeb.js"><link rel="prefetch" href="/assets/js/87.9c24da6f.js"><link rel="prefetch" href="/assets/js/88.bcbf388d.js"><link rel="prefetch" href="/assets/js/89.e3aaff55.js"><link rel="prefetch" href="/assets/js/9.3f4a2127.js"><link rel="prefetch" href="/assets/js/90.23eda780.js"><link rel="prefetch" href="/assets/js/91.da84bead.js"><link rel="prefetch" href="/assets/js/92.7d2a6af1.js"><link rel="prefetch" href="/assets/js/93.4de0373f.js">
    <link rel="stylesheet" href="/assets/css/styles.c0a0368e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div><a href="/" class="home-link router-link-active"><!----><span class="site-name">
      Information Theory
    </span></a><div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""><!----></div><!----></div></header><div class="sidebar-mask"></div><div class="sidebar"><!----><ul class="sidebar-links"><li><div class="sidebar-group first collapsable"><p class="sidebar-heading"><span>general information</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>01 probability theory</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>02 entropy</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>03 source coding data compression</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>04 typical sets and encryption</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>05 random processes</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>06 error correction and zero error transmission</span><span class="arrow right"></span></p><!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading open"><span>07 noisy channel coding</span><span class="arrow down"></span></p><ul class="sidebar-group-items"><li><a href="/07-noisy-channel-coding/definition-achievable-rate/" class="sidebar-link">Definition: Achievable Rate</a></li><li><a href="/07-noisy-channel-coding/mathematical-tools/fanos-inequality/" class="sidebar-link">Fano's Inequality</a></li><li><a href="/07-noisy-channel-coding/mathematical-tools/definition-joint-typicality/" class="sidebar-link">Definition: Joint Typicality</a></li><li><a href="/07-noisy-channel-coding/mathematical-tools/joint-asymptotic-equipartition-property-joint-aep/" class="sidebar-link">Joint Asymptotic Equipartition Property (Joint AEP)</a></li><li><a href="/07-noisy-channel-coding/shannons-noisy-channel-coding-theorem/noisy-channel-theorem-forward-direction/" class="active sidebar-link">Noisy-Channel Theorem: Forward Direction</a></li><li><a href="/07-noisy-channel-coding/shannons-noisy-channel-coding-theorem/noisy-channel-theorem-converse/" class="sidebar-link">Noisy-Channel Theorem: Converse</a></li><li><a href="/07-noisy-channel-coding/source-channel-separation/source-channel-coding/" class="sidebar-link">Source-Channel Coding</a></li><li><a href="/07-noisy-channel-coding/source-channel-separation/source-channel-separation-theorem-forward-direction/" class="sidebar-link">Source-Channel Separation Theorem: Forward Direction</a></li><li><a href="/07-noisy-channel-coding/source-channel-separation/source-channel-separation-theorem-converse/" class="sidebar-link">Source-Channel Separation Theorem: Converse</a></li></ul></div></li></ul></div><div class="page"><div class="content"><h1 id="noisy-channel-theorem-forward-direction"><a href="#noisy-channel-theorem-forward-direction" aria-hidden="true" class="header-anchor">#</a> Noisy-Channel Theorem: Forward Direction</h1><p>With the tools from the previous section, we are ready to prove the forward direction of Shannon's noisy-channel coding theorem, which states that any rate strictly below the channel capacity is achievable:</p><div class="content-box pad-box-mini border border-trbl border-round"><h4 style="color: #bc0031;"><strong>Theorem: Shannon's noisy-channel coding theorem (forward direction)</strong></h4>
For a discrete memoryless channel with capacity \(C\), any rate \(R &lt; C\) is achievable. Concretely, for any \( \varepsilon &gt; 0 \) and any rate \( R &lt; C \), for large enough \( n \) there exists a \( (2^{n \cdot R}, n) \) code with <a title="Definitions: Code, Rate, and Error Probability" href="https://canvas.uva.nl/courses/2205/pages/definitions-code-rate-and-error-probability" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/definitions-code-rate-and-error-probability" data-api-returntype="Page">maximal error</a> \( \lambda^{(n)} &lt; \varepsilon \).
<p><span role="button" aria-controls="group6" aria-label="Toggler" aria-expanded="false" class="element_toggler"><span class="Button">Proof</span></span></p><div id="group6"><div class="content-box"><p>Given a channel \((\mathcal{X},P_{Y|X},\mathcal{Y})\) with capacity \(C = \max{P_X} I(X;Y)\), let \(R &lt; C\) and \(\epsilon &gt; 0\). We will first show that for big enough \(n\), a <i>randomly constructed</i> code with rate \(R\) has a low error probability. We will then argue that a low error probability on average of codes implies the existence of some <i>specific</i> code with low error probability.</p><p>Fix an input distribution \(P_X\) that maximizes \(I(X;Y)\). For any \(n\), construct a <a title="2^{nR} is not necessarily an integer. In an exercise, you will be asked to consider the case where it is not." data-tooltip="{"tooltipClass":"popover popover-padded", "position":"right"}">\((2^{n \cdot R},n)\)-code</a> \(\mathcal{C}\) by choosing a codebook at random according to \(P_X\). That is, for every message \(w \in [2^{n \cdot R}]\), sample \(n\) times from the distribution \(P_X\), creating a codeword \(\mathcal{C}(w) = (\mathcal{C}_1(w), \mathcal{C}_2(w), ..., \mathcal{C}_n(w))\) by concatenating the \(n\) independent samples \(\mathcal{C}_i(w) \sim P_X\).</p><p>Since the channel is memoryless, if \(w\) is sent over the channel using \(\mathcal{C}\), the output distribution \(Y^n\) is given by: \begin{align} P_{Y^n|X^n}(y^n|\mathcal{C}(w)) = \prod_{i=1}^n P_{Y|X} (y_i \mid \mathcal{C}_i(w)). \end{align} What is the probability that the decoded message \(\hat{w}\) is incorrect, i.e., not equal to \(w\)? This depends on the decoding method used by the receiver. The optimal decoding procedure is <span style="color: #bc0031;"><strong>maximum-likelihood decoding</strong></span>, where the input message that is most likely with respect to \(P_{X|Y}\) is selected as the decoding \(\hat{w}\). However, it is hard to analyze the error probability for this decoding method. Instead, we will assume that the receiver applies <span style="color: #bc0031;"><strong>jointly typical decoding</strong></span>, which has a slightly higher probability of decoding to the wrong message, but still small enough for our analysis. Jointly typical decoding works as follows: upon receiving an output \(y^n\), the receiver looks for a <i>unique</i> message \(\hat{w}\) such that the pair \((\mathcal{C}(\hat{w}),y^n)\) is jointly typical. If there exists no such message, or if it is not unique, the receiver declares a failure by decoding to \(\hat{w} = 0\) (which is always wrong because \(w \in [2^{n\cdot R}] = \{1,2,\ldots,2^{n\cdot R} \} \) ).</p><p>With this decoding procedure in mind, we analyze the average error probability \(P[\texttt{error}]\), where the average is taken over both the randomly constructed code \(\mathcal{C}\) and the uniformly randomly selected message \(w\). Defining \(\lambda_w(\mathcal{C}) := P[\hat{w} \neq w \mid \mathcal{C}(w) \text{ was sent over the channel}]\) to be the probability that a message \(w\) (encoded using \(\mathcal{C}\)) is decoded incorrectly, we get: \begin{align} P[\texttt{error}] &amp;= \sum_{\mathcal{C}} P[\mathcal{C}] \cdot \left(\sum_{w=1}^{2^{n\cdot R}} \frac{1}{2^{n \cdot R}} \cdot \lambda_w(\mathcal{C}) \right) \\&amp;= \frac{1}{2^{n \cdot R}} \sum_{w = 1}^{2^{n \cdot R}} \sum_{\mathcal{C}} P[\mathcal{C}] \cdot \lambda_w(\mathcal{C}). \end{align} Since we average over all randomly constructed codes \(\mathcal{C}\), and the codewords for all messages are sampled independently, the value \(\sum_{\mathcal{C}} P[\mathcal{C}] \cdot \lambda_w(\mathcal{C})\) does not depend on the particular message \(w\). Hence if we set, for example, \(w_0 = 1\), then for all \(w \in [2^{n \cdot R}]\), \begin{align} \sum_{\mathcal{C}} P[\mathcal{C}] \lambda_w(\mathcal{C}) = \sum_{\mathcal{C}} P[\mathcal{C}] \lambda_{w_0}(\mathcal{C}). \end{align} This simplifies the calculation of \(P[\texttt{error}]\) significantly: \begin{align} P[\texttt{error}] &amp;= \frac{1}{2^{n \cdot R}} \sum_{w = 1}^{2^{n \cdot R}} \sum_{\mathcal{C}} P[\mathcal{C}] \cdot \lambda_{w_0}(\mathcal{C}) \\&amp;= \sum_{\mathcal{C}} P[\mathcal{C}] \cdot \lambda_{w_0}(\mathcal{C}). \end{align} That is, the average probability of error is the probability (over the selection of the code \(\mathcal{C}\), and over the randomness in the channel) that the message \(w_0\) is decoded incorrectly. There are two possible reasons for an error in the decoding:</p><ol><li>The output of the channel is not jointly typical with \(\mathcal{C}(w_0)\). By the first item of the <a title="Joint Asymptotic Equipartition Property (Joint AEP)" href="https://canvas.uva.nl/courses/2205/pages/joint-asymptotic-equipartition-property-joint-aep" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/joint-asymptotic-equipartition-property-joint-aep" data-api-returntype="Page">joint AEP</a>, this probability approaches zero as \(n\) goes to infinity. Hence, for big enough \(n\), the probability of an error for this reason is smaller than \(\epsilon\).</li><li>There is some \(w' \neq w_0\) such that the output of the channel is (also) jointly typical with \(\mathcal{C}(w')\). Since \(\mathcal{C}\) is a random code (and so \(\mathcal{C}(w')\) is independent from the channel output \(y^n\)), by the third item of the <a title="Joint Asymptotic Equipartition Property (Joint AEP)" href="https://canvas.uva.nl/courses/2205/pages/joint-asymptotic-equipartition-property-joint-aep" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/joint-asymptotic-equipartition-property-joint-aep" data-api-returntype="Page">joint AEP</a> the probability that this occurs is at most \begin{align} \sum_{w' \neq w_0} 2^{-n (I(X;Y) - 3 \epsilon)} = (2^{n \cdot R} - 1) 2^{-n (I(X;Y) - 3 \epsilon)}. \end{align}</li></ol><p>We can thus bound the average probability of error, using the union bound and the bounds in the above analysis, by \begin{align} P[\texttt{error}] &amp;\leq \epsilon + (2^{n \cdot R} - 1) 2^{-n (I(X;Y) - 3 \epsilon)} \\&amp;\leq \epsilon + 2^{n \cdot R} 2^{-n (I(X;Y) - 3 \epsilon)} \\&amp;= \epsilon + 2^{-n (I(X;Y) - R - 3 \epsilon)} \end{align} As long as \(R &lt; I(X;Y)\), one can choose \(n\) large enough so that \(P[\texttt{error}] \leq 2 \epsilon\).</p><p>This analysis upper bounds the (expected) average error probability for a random code \(\mathcal{C}\). However, if this expected probability is low, there must be some specific code \(\mathcal{C}^*\) that also has low average error probability.</p><p>Finally, in \(\mathcal{C}^*\), we aim to bound the <i>maximal</i> error probability, i.e., the probability of error for the worst message. We can do so by noting that at least half of the messages \(w\) has error probability \(\lambda_w(\mathcal{C}^*) \leq 4 \epsilon\): if not, then the total error probability of these messages would already exceed \(2^{n \cdot R} \cdot 2\epsilon\), contradicting the upper bound of \(2 \epsilon\) to the average error probability. Thus, we can construct a better code by discarding the worst half of the codewords, and using the remaining \(2^{n \cdot R - 1}\) codewords to construct a new code, with rate \begin{align} \frac{\log(2^{n\cdot R - 1})}{n} = \frac{n \cdot R - 1}{n} = R - \frac{1}{n} \end{align} and maximal probability of error \(\lambda^{(n)} \leq 4 \epsilon\).</p></div></div></div><p>In the above proof, we implicitly assumed that \(2^{n\cdot R}\) is an integer. You can try to redo the proof for the case when it is not: construct \(\mathcal{C}\) as a \((\lceil 2^{n \cdot R}\rceil, n)\) code, and verify that the average probability of error \(P[\texttt{error}]\) is still sufficiently small. Also compute a lower bound on the rate of the final code.</p></div><!----><div class="content page-nav"><p class="inner"><span class="prev">
        ← <a href="/07-noisy-channel-coding/mathematical-tools/joint-asymptotic-equipartition-property-joint-aep/" class="prev">
          Joint Asymptotic Equipartition Property (Joint AEP)
        </a></span><span class="next"><a href="/07-noisy-channel-coding/shannons-noisy-channel-coding-theorem/noisy-channel-theorem-converse/">
          Noisy-Channel Theorem: Converse
        </a> →
      </span></p></div></div></div></div>
    <script src="/assets/js/84.f86545fe.js" defer></script><script src="/assets/js/app.c0a0368e.js" defer></script>
  </body>
</html>
