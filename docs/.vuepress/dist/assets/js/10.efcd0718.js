(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{80:function(t,e,i){"use strict";i.r(e);var r=i(0),n=Object(r.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("div",{staticClass:"content"},[i("h1",{attrs:{id:"definition-relative-entropy"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#definition-relative-entropy","aria-hidden":"true"}},[t._v("#")]),t._v(" Definition: Relative Entropy")]),i("p",[t._v("We can compare two distributions on the same set \\(\\mathcal{X}\\) by considering their relative entropy: this measure reflects how different two distributions are.")]),i("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[i("h4",{staticStyle:{color:"#bc0031"}},[i("strong",[t._v("Definition: Relative entropy")])]),t._v("\nThe relative entropy (or: "),i("a",{attrs:{href:"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"}},[i("span",{staticStyle:{color:"#bc0031"}},[i("strong",[t._v("Kullback-Leibler divergence")])])]),t._v(") of two probability distributions \\(P\\) and \\(Q\\) over the same \\(\\mathcal{X}\\) is defined by \\[ D(P||Q) := \\sum_{\\substack{x \\in \\mathcal{X} P(x) > 0}} P(x) \\log \\frac{P(x)}{Q(x)}, \\] where by convention, \\(\\log\\frac{p}{0} = \\infty\\) for all \\(p\\).")]),i("p",[t._v("Note that if \\(Q(x) = 0\\) for some \\(x\\) with \\(P(x) > 0\\), then \\(D(P||Q) = \\infty\\).")])])}],!1,null,null,null);n.options.__file="README.md";e.default=n.exports}}]);