(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{128:function(t,a,e){"use strict";e.r(a);var n=e(0),i=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("div",{staticClass:"content"},[e("h1",{attrs:{id:"properties-of-relative-entropy"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#properties-of-relative-entropy","aria-hidden":"true"}},[t._v("#")]),t._v(" Properties of Relative Entropy")]),e("p",[t._v("Even though relative entropy is always nonnegative (see the theorem below), it is not a proper distance measure, because it is not symmetric and does not satisfy the triangle inequality.")]),e("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[e("h4",{staticStyle:{color:"#bc0031"}},[e("strong",[t._v("Lemma: Alternative Definition of Mutual Information"),e("br")])]),t._v("\nThe mutual information between \\(X\\) and \\(Y\\) can be expressed in terms of the relative entropy of their distributions as follows: \\[ I(X;Y) = D(P_{XY} || P_X \\cdot P_Y) \\]\n"),e("p",[e("span",{staticClass:"element_toggler",attrs:{role:"button","aria-controls":"group1","aria-label":"Toggler","aria-expanded":"false"}},[e("span",{staticClass:"Button"},[t._v("Proof")])])]),e("div",{attrs:{id:"group1"}},[e("div",{staticClass:"content-box"},[t._v("The statement follows by writing out the definitions of mutual information and relative entropy, and rearranging terms. \\begin{align} I(X;Y) &= H(X) - H(X|Y)\\\\ &= - \\sum_{x \\in \\mathcal{X}} P_X(x) \\log P_X(x) + \\sum_{y \\in \\mathcal{Y}} P_Y(y) \\sum_{x \\in \\mathcal{X}} P_{X|Y}(x|y) \\log P_{X|Y}(x|y)\\\\ &= - \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} P_{XY}(x,y) \\log P_X(x)+ \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}} P_{XY}(x,y) \\log P_{X|Y}(x|y)\\\\ &= - \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}: P_{XY}(x,y) > 0} P_{XY}(x,y) \\log P_X(x)+ \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y} : P_{XY}(x,y) > 0} P_{XY}(x,y) \\log P_{X|Y}(x|y)\\\\ &= \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}: P_{XY}(x,y) > 0} P_{XY}(x,y) (-\\log P_X(x)+\\log P_{X|Y}(x|y))\\\\ &= \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}: P_{XY}(x,y) > 0} P_{XY}(x,y) \\log \\frac{P_{X|Y}(x|y)}{P_X(x)}\\\\ &= \\sum_{x \\in \\mathcal{X}, y \\in \\mathcal{Y}: P_{XY}(x,y) > 0} P_{XY}(x,y) \\log \\frac{P_{XY}(x,y)}{P_X(x)P_Y(y)}\\\\ &= D(P_{XY}||P_XP_Y) \\end{align}")])])]),e("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[e("h4",{staticStyle:{color:"#bc0031"}},[e("strong",[t._v("Theorem: Information Inequality")])]),t._v("\nFor any two probability distributions \\(P\\) and \\(Q\\) defined on the same \\(\\mathcal{X}\\), \\[ D(P||Q) \\geq 0. \\] Equality holds if and only if \\(P = Q\\).\n"),e("p",[e("span",{staticClass:"element_toggler",attrs:{role:"button","aria-controls":"group2","aria-label":"Toggler","aria-expanded":"false"}},[e("span",{staticClass:"Button"},[t._v("Proof")])])]),e("div",{attrs:{id:"group2"}},[e("div",{staticClass:"content-box"},[t._v("Left as an exercise. Hint: use Jensen's inequality.")])])]),e("p",[t._v("The above lemma and theorem together show that the mutual information is a measure of 'how independent' the variables \\(X\\) and \\(Y\\) are: if \\(P_{XY} = P_X \\cdot P_Y\\), the variables are independent and their mutual information is zero.")])])}],!1,null,null,null);i.options.__file="README.md";a.default=i.exports}}]);