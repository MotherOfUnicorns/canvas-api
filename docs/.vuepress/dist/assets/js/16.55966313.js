(window.webpackJsonp=window.webpackJsonp||[]).push([[16],{89:function(t,a,e){"use strict";e.r(a);var i=e(0),r=Object(i.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("div",{staticClass:"content"},[e("h1",{attrs:{id:"definition-binary-entropy"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#definition-binary-entropy","aria-hidden":"true"}},[t._v("#")]),t._v(" Definition: Binary Entropy")]),e("p",[t._v("For a "),e("a",{attrs:{title:"A binary random variable is a random variable that can take on two values.","data-tooltip":'{"tooltipClass":"popover popover-padded", "position":"right"}'}},[t._v("binary random variable")]),t._v(" \\(X\\) with image \\({\\cal X} = \\{x_0,x_1\\}\\) and probabilities \\(P_X(x_0) = p\\) and \\(P_X(x_1) = 1-p\\), we can write \\(H(X) = h(p)\\), where \\(h\\) denotes the binary entropy function:")]),e("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[e("h4",{staticStyle:{color:"#bc0031"}},[e("strong",[t._v("Definition: Binary entropy function \\(h\\)")])]),e("p",[t._v("The binary entropy function is defined for \\(0 < p < 1\\) as \\[ h(p) := p \\log\\frac{1}{p} + (1-p)\\log\\frac{1}{1-p}, \\] and is defined as \\(h(p) = 0\\) for \\(p=0\\) or \\(p=1\\).")])]),e("p",[t._v("The "),e("a",{attrs:{href:"https://www.wolframalpha.com/input/?i=Plot%5B-p*log2(p)-(1-p)*log2(1-p),+p%3D0..1%5D"}},[t._v("graph of \\(h\\) ")]),t._v("on the interval \\([0,1]\\), as a function of \\(p\\), looks as follows:")]),e("p",{staticStyle:{"text-align":"center"}},[e("a",{staticClass:"instructure_file_link",attrs:{title:"fig-binary-entropy-1.svg",href:"https://canvas.uva.nl/courses/2205/files/46125/download?verifier=S390kKrlAnalczK4QkRCvq5f1CJ7JectfRO2Yjy1&wrap=1","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/files/46125","data-api-returntype":"File"}},[e("img",{staticStyle:{border:"0px solid #000000",padding:"2px"},attrs:{src:"https://canvas.uva.nl/courses/2205/files/46125/preview?verifier=S390kKrlAnalczK4QkRCvq5f1CJ7JectfRO2Yjy1",alt:"plot of binary entropy",width:"318",height:"254","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/files/46125","data-api-returntype":"File"}})])]),e("p",{staticStyle:{"text-align":"left"}},[t._v("If weÂ think of \\(X\\) as the random variable describing the outcome of a coin flip, we see that a relatively fair coin (\\(p \\approx \\frac{1}{2}\\)) yields a higher expected surprisal value than a very biased coin (where \\(p\\) is closer to 0 or 1). If the coin is completely fair (\\(p = \\frac{1}{2}\\)), the entropy is exactly 1 bit.")]),e("p")])}],!1,null,null,null);r.options.__file="README.md";a.default=r.exports}}]);