(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{101:function(e,l,n){"use strict";n.r(l);var t=n(0),a=Object(t.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var e=this,l=e.$createElement,n=e._self._c||l;return n("div",{staticClass:"content"},[n("h1",{attrs:{id:"theorem-shannon-s-source-coding-theorem-optimal-codes"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#theorem-shannon-s-source-coding-theorem-optimal-codes","aria-hidden":"true"}},[e._v("#")]),e._v(" Theorem: Shannon's Source-Coding Theorem (Optimal Codes)")]),n("p",[e._v("We now know that prefix-free codes can achieve the same minimal code lengths for a source \\(P_X\\) as the more general class of uniquely decodable codes. How small is this minimal code length in general? In this section we explore the following relation between the minimal code length and the entropy of the source:")]),n("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[n("h4",{staticStyle:{color:"#bc0031"}},[n("strong",[e._v("Theorem: Shannon's source-coding theorem (for symbol codes)")])]),e._v("\nFor any source \\(P_X\\), we have the following bounds: \\[ H(X) \\leq \\ell_{\\min}(P_X) \\leq H(X) + 1. \\]\n"),n("p",[n("span",{staticClass:"element_toggler",attrs:{role:"button","aria-controls":"group7","aria-label":"Toggler","aria-expanded":"false"}},[n("span",{staticClass:"Button"},[e._v("\\(H(X) \\leq \\ell_{\\min}(P_X)\\)")])])]),n("div",{attrs:{id:"group7"}},[n("div",{staticClass:"content-box"},[e._v("The proof relies on Kraft's inequality. Let \\(C\\) be a code, and write \\(\\ell_x\\) for \\(\\ell(C(x))\\) as a notational convenience. For the lower bound, we have that \\begin{align} H(X) - \\ell_C(P_X) &= - \\sum_{x \\in {\\cal X}} P_X(x) \\log(P_X(x)) - \\sum_{x \\in X} P_X(x) \\ell_x\\\\ &= \\sum_{x \\in {\\cal X}} P_X(x) \\left(-\\log(P_X(x)) - \\log\\left(2^{\\ell_x}\\right)\\right)\\\\ &= \\sum_{x \\in {\\cal X}} P_X(x) \\log \\left(\\frac{1}{P_X(x)\\cdot 2^{\\ell_x}}\\right)\\\\ &\\leq \\log\\left(\\sum_{x \\in {\\cal X}} \\frac{1}{2^{\\ell_x}}\\right) &&\\mbox{(by Jensen's inequality)}\\\\ &\\leq \\log(1) = 0&&\\mbox{(by Kraft's inequality)} \\end{align}")])]),n("p",[n("span",{staticClass:"element_toggler",attrs:{role:"button","aria-controls":"group8","aria-label":"Toggler","aria-expanded":"false"}},[n("span",{staticClass:"Button"},[e._v("\\(\\ell_{\\min}(P_X) \\leq H(X) + 1\\)")])])]),n("div",{attrs:{id:"group8"}},[n("div",{staticClass:"content-box"},[e._v("For the upper bound, let us denote by \\(\\ell_x\\) the surprisal value in bits rounded up to the next integer, i.e. for any \\(x \\in {\\cal X}\\),\n\\begin{align}\n\\ell_x := \\left\\lceil\\log\\frac{1}{P_X(x)}\\right\\rceil,\n\\end{align}\nand note that\n\\begin{align}\n\\sum_{x \\in {\\cal X}} 2^{-\\ell_x} \\leq \\sum_{x \\in {\\cal X}} 2^{-\\log\\frac{1}{P_X(x)}} = \\sum_{x \\in {\\cal X}} P_X(x) = 1.\n\\end{align}\nTherefore, by Kraft's inequality, there exists a prefix-free code \\(C\\) such that \\(\\ell(C(x)) = \\ell_x\\) for all \\(x \\in {\\cal X}\\). This code satisfies\n\\begin{align}\n\\ell_C(P_X) &= \\sum_{x \\in {\\cal X}} P_X(x) \\ell_x \\\\\n&\\leq \\sum_{x \\in {\\cal X}} P_X(x) \\left(\\log \\frac{1}{P_X(x)} + 1 \\right)\\\\\n&= -\\sum_{x \\in {\\cal X}} P_X(x)\\log P_X(x) + \\sum_{x \\in {\\cal X}} P_X(x) \\\\\n&= H(X) + 1.\n\\end{align}\nWe have thus constructed a code \\(C\\) with \\(\\ell_C(P_X) \\leq H(X) + 1\\), so \\(\\ell_{\\min}(P_X) \\leq H(X) + 1\\).\n")])])])])}],!1,null,null,null);a.options.__file="README.md";l.default=a.exports}}]);