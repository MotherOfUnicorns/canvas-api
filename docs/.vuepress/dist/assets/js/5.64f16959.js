(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{74:function(a,t,e){"use strict";e.r(t);var i=e(0),n=Object(i.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var a=this,t=a.$createElement,e=a._self._c||t;return e("div",{staticClass:"content"},[e("h1",{attrs:{id:"random-variables-and-distributions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#random-variables-and-distributions","aria-hidden":"true"}},[a._v("#")]),a._v(" Random Variables and Distributions")]),e("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[e("h4",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("Definition: Discrete Random Variable (RV)")])]),a._v("\nLet \\((\\Omega, \\mathcal{F}, P)\\) be a discrete probability space. A random variable \\(X\\) is a function \\(X : \\Omega \\to \\mathcal{X}\\) where \\(\\mathcal{X}\\) is a set, and we may assume it to be discrete.")]),e("p",[a._v("A "),e("i",[a._v("real")]),a._v(" random variable is one whose image is contained in \\(\\mathbb{R}\\). A (The "),e("i",[a._v("image")]),a._v(" and the "),e("i",[a._v("range")]),a._v(" of a random variable \\(X\\) are given by the image and the range of \\(X\\) in the function-theoretic sense.) The image of a "),e("i",[a._v("binary")]),a._v(" random variable is a set \\({x_0, x_1}\\) with only two elements.")]),e("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[e("h4",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("Definition: Probability distribution")])]),a._v("\nLet \\(X\\) be a random variable. The probability distribution of \\(X\\) is the function \\(P_X : \\mathcal{X} \\to [0,1]\\) defined as \\[ P_X(x) := P[X = x], \\] where \\(X = x\\) denotes the event \\(\\{\\omega \\in \\Omega \\mid X(\\omega) = x\\}\\).")]),e("p",[a._v("Alternatively, one can write \\(P_X(x) = P[X^{-1}(x)]\\) to express that the probability of \\(x\\) is precisely the \\(P\\)-measure of the pre-image of \\(x\\) under the random variable \\(X\\).")]),e("p",[a._v("We say that \\(P_X\\) is a "),e("span",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("uniform")])]),a._v(" distribution if the associated probability measure is uniform, i.e. \\(P_X(x) = \\frac{1}{|\\mathcal{X}|}\\). The "),e("span",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("support")])]),a._v(" of a random variable or a probability distribution is defined as \\(\\text{supp}(P_X) := \\{x \\in \\mathcal{X} \\mid P_X(x) > 0\\}\\), the points of the range which have strictly positive probability. We often slightly abuse notation and write \\(\\text{supp}(X)\\) instead. When given two or more random variables defined on the same probability space, we can consider the probability that each of the variables take on a certain value:")]),e("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[e("h4",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("Definition: Joint probability distribution")])]),a._v("\nLet \\(X\\) and \\(Y\\) be two random variables defined on the same probability space, with respective ranges \\(\\mathcal{X}\\) and \\(\\mathcal{Y}\\). The pair \\(XY\\) is a random variable with probability distribution \\(P_{XY} : \\mathcal{X} \\times \\mathcal{Y} \\to [0,1]\\) given by \\[ P_{XY}(x,y) := P[X = x, Y = y]. \\]")]),e("p",[a._v("This definition naturally extends to three and more random variables. Unless otherwise stated, a collection of random variables is assumed to be defined on the same (implicit) probability space, so that their joint distribution is always well-defined. If \\(P_{XY} = P_X \\cdot P_Y\\), in the sense that \\(P_{XY}(x,y) = P_X(x)P_Y(y)\\) for all \\(x \\in \\mathcal{X}\\) and \\(y \\in \\mathcal{Y}\\), then the random variables \\(X\\) and \\(Y\\) are said to be "),e("span",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("independent")])]),a._v(". If a set of variables \\(X_1, \\ldots, X_n\\) are all mutually independent and all have the same distribution (i.e., \\(P_{X_i} = P_{X_j}\\) for all \\(i,j\\)), then they are "),e("span",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("independent and identically distributed")])]),a._v(", or "),e("span",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("i.i.d.")])]),a._v(" From a joint distribution, we can always find out the \"original'' (or "),e("span",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("marginal")])]),a._v(") distribution of one of the random variables (for example, \\(X\\)) by "),e("span",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("marginalizing")])]),a._v(" out the variable that we want to discard (for example, \\(Y\\)): \\[ P_X(x) = \\sum_{y \\in \\mathcal{Y}} P_{XY}(x,y). \\] This marginalization process also works with more than two random variables. Like events, probability distributions can also be conditioned on probabilistic events:")]),e("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[e("h4",{staticStyle:{color:"#bc0031"}},[e("strong",[a._v("Definition: Conditional probability distribution")])]),a._v("\nIf \\(\\mathcal{A}\\) is an event with \\(P[\\mathcal{A}] > 0\\), then the conditional probability distribution of \\(X\\) given \\(\\mathcal{A}\\) is given by \\[ P_{X|\\mathcal{A}}(x) = \\frac{P[X=x, \\mathcal{A}]}{P[\\mathcal{A}]}. \\] If \\(Y\\) is another random variable and \\(P_Y(y) > 0\\), then we write \\[ P_{X | Y}(x| y) := P_{X | Y = y}(x) = \\frac{P_{XY}(x,y)}{P_Y(y)} \\] for the conditional distribution of \\(X\\), given \\(Y = y\\).")]),e("p",[a._v("Note that again, both \\((\\mathcal{X},P_{X | \\mathcal{A}})\\) and \\((\\mathcal{X},P_{X| Y=y})\\) themselves form probability spaces. Note also that if \\(X\\) and \\(Y\\) are independent, then \\[ P_{X | Y}(x |y) = \\frac{P_{XY}(x,y)}{P_Y(y)} = \\frac{P_X(x) \\cdot P_Y(y)}{P_Y(y)} = P_X(x), \\] which aligns well with our intuition of independent variables: the distribution of \\(X\\) remains unchanged when \\(Y\\) is fixed to a specific value.")]),e("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[e("h4",{staticStyle:{color:"#2d3b45"}},[e("strong",[a._v("Example: Fair die (continued)")])]),a._v("\nConsider again the throw of a six-sided fair die. Let the random variable \\(X\\) describe the number of (distinct) integer divisors for the outcome, that is \\[X(1) = 1 \\ \\ \\ \\ \\ X(2) = 2 \\ \\ \\ \\ \\ X(3) = 2 \\ \\ \\ \\ \\ X(4) = 3 \\ \\ \\ \\ \\ X(5) = 2 \\ \\ \\ \\ \\ X(6) = 4 \\] \\(X\\) is a real random variable, with range \\(\\mathcal{X} = {1,2,3,4}\\). The associated probability distribution is \\[P_X(1) = P[{1}] = \\frac{1}{6}, \\hspace{4mm} P_X(2) = P[{2,3,5}] = \\frac{1}{2}, \\hspace{4mm} P_X(3) = P[{4}] = \\frac{1}{6}, \\hspace{4mm} P_X(4) = P[{6}]=\\frac{1}{6} \\, . \\] If we now condition on the event \\(\\mathcal{A} = {2,4,6}\\) (the outcome of the die being even), we get that \\[P_{X | \\mathcal{A}}(1) = 0, \\hspace{6mm} P_{X | \\mathcal{A}}(2) = \\frac{1}{3}, \\hspace{6mm} P_{X | \\mathcal{A}}(3) = \\frac{1}{3}, \\hspace{6mm} P_{X | \\mathcal{A}}(4) = \\frac{1}{3} \\]")]),e("p",[a._v("If \\(X\\) is a random variable and \\(f : \\mathcal{X} \\to \\mathcal{Y}\\) is a surjective function, then \\(f(X)\\) is a random variable, defined by composing the map \\(f\\) with the map \\(X\\). Its image is \\(\\mathcal{Y}\\). Clearly, \\[ P_{f(X)}(y) = \\sum_{x \\in \\mathcal{X} : f(x) = y} P_X(x). \\] For example, \\(1/P_X(X)\\) denotes the real random variable obtained from another random variable \\(X\\) by composing with the map \\(1/P_X\\) that assigns \\(1/P_X(x) \\in \\mathbb{R}\\) to \\(x \\in \\mathcal{X}\\).")])])}],!1,null,null,null);n.options.__file="README.md";t.default=n.exports}}]);