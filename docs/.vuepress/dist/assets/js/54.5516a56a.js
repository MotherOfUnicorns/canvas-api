(window.webpackJsonp=window.webpackJsonp||[]).push([[54],{93:function(i,t,a){"use strict";a.r(t);var e=a(0),r=Object(e.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var i=this,t=i.$createElement,a=i._self._c||t;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"random-walks-on-graphs"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#random-walks-on-graphs","aria-hidden":"true"}},[i._v("#")]),i._v(" Random Walks on Graphs")]),a("p",[i._v("An important and widely applicable example of a time-invariant Markov process is a random walk on a connected graph G with strictly positive symmetric edge weights \\( W_{ij} = W_{ji} \\). The random walk is defined as follows: at node \\( i \\), walk to node \\( j \\) with probability \\( \\frac{W_{ij}}{W_i} \\) where \\( W_i := \\sum_j W_{ij} \\) is the sum of the weights of all edges involving node \\( i \\), and \\( W := \\frac12 \\sum_i W_i \\) is the total of all edge weights.")]),a("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[a("h4",{staticStyle:{color:"#2d3b45"}},[a("strong",[i._v("Example")])]),i._v("\nFor the following graph"),a("img",{attrs:{src:"https://canvas.uva.nl/courses/2205/files/603108/preview?verifier=S0DPRimWnSuQET3SX4nuwqpUlNfSh3HBthk4UDUs",alt:"RandomWalkGraph.png",width:"200",height:"144","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/files/603108","data-api-returntype":"File"}}),i._v(", we have that \\( W_1 = 2, W_2 = 4, W_3 = 3, W_4 = 5 \\) and \\( 2 \\cdot W = \\sum_i W_i = 2 \\cdot 7 \\).")]),a("p",[i._v("The stationary distribution of this random walk is given by \\( \\mu_i := \\frac{W_i}{2W} \\), because indeed, at every node \\( i \\), we have that the sum of all incoming weight is")]),a("p",[i._v("\\[ \\sum_j \\mu_j \\frac{W_{ij}}{W_j} = \\sum_j \\frac{W_j}{2W} \\frac{W_{ij}}{W_j} = \\frac{W_i}{2W} = \\mu_i \\, .\\]")]),a("p",[i._v("We continue to compute the entropy rate of this random walk. Assuming we start in the stationary distribution, we can compute the entropy rate as follows.")]),a("p",[i._v("\\begin{align} H( \\{ X_i \\} ) = \\sum_i \\mu_i H( \\ldots \\frac{W_{ij}}{W_i} \\ldots ) &= - \\sum_i \\mu_i \\sum_j \\frac{W_{ij}}{W_i} \\log \\frac{W_{ij}}{W_i} \\\\ &= - \\sum_{i,j} \\frac{ W_i} {2W} \\cdot \\frac{ W_{ij} }{W_i} \\log \\left( \\frac{W_{ij}}{2W} \\cdot \\frac{2W}{W_i} \\right) \\\\ &= - \\sum_{i,j} \\frac{ W_{ij}} {2W} \\log \\left( \\frac{W_{ij}}{2W} \\right) + \\sum_{i,j} \\frac{ W_{ij}} {2W} \\log \\left( \\frac{W_{i}}{2W} \\right) \\\\ &= - \\sum_{i,j} \\frac{ W_{ij}} {2W} \\log \\left( \\frac{W_{ij}}{2W} \\right) + \\sum_{i} \\frac{ W_{i}} {2W} \\log \\left( \\frac{W_{i}}{2W} \\right) \\\\ &= H( \\ldots \\frac{W_{ij}}{2W} \\ldots) - H( \\ldots \\frac{W_i}{2W} \\ldots) \\, \\end{align} which is the difference of the entropy of the edge distribution and the entropy of the stationary distribution.")]),a("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[a("h4",{staticStyle:{color:"#2d3b45"}},[a("strong",[i._v("Example, continued")])]),a("p",[i._v("In the example above, the edge distribution is \\( \\frac{1}{14} (1,1,1,2,2,1,1,1,2,2) \\) and the stationary distribution is \\( \\frac{1}{14}(2,4,3,5) \\), resulting in \\( H(\\{ X_i \\}) = H( \\frac{1}{14} (1,1,1,2,2,1,1,1,2,2) ) - H( \\frac{1}{14}(2,4,3,5) ) \\href{https://www.wolframalpha.com/input/?i=-6+*+1%2F14+*+log2(1%2F14)+-+4+*+2%2F14+*+log2(2%2F14)+-+(+-2%2F14*log2(2%2F14)+-+3%2F14*log2(3%2F14)+-+4%2F14*log2(4%2F14)+-+5%2F14*log2(5%2F14))}{\\approx} 1.312\\)")])])])}],!1,null,null,null);r.options.__file="README.md";t.default=r.exports}}]);