(window.webpackJsonp=window.webpackJsonp||[]).push([[57],{84:function(t,e,a){"use strict";a.r(e);var i=a(0),s=Object(i.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"markov-process-stationary-distribution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#markov-process-stationary-distribution","aria-hidden":"true"}},[t._v("#")]),t._v(" Markov Process: Stationary Distribution")]),a("p",[a("img",{attrs:{src:"https://canvas.uva.nl/courses/2205/files/413835/preview?verifier=d8BwJdQOyZ4E7qED9VJvB2wvo3vjyweCdjfteTkB",alt:"RandomProcess.png",width:"600",height:"488","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/files/413835","data-api-returntype":"File"}})]),a("p",[t._v("Suppose that we run the process of "),a("a",{attrs:{href:"https://canvas.uva.nl/courses/2205/pages/markov-process-time-invariance-finite-state-transition-matrix#example2","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/pages/markov-process-time-invariance-finite-state-transition-matrix%23example2","data-api-returntype":"Page"}},[t._v("Example 2")]),t._v(" for a very large number of steps, and wonder what the probability will be of observing an \\(\\texttt{a}\\) at the next step. Given the initial distribution and the state diagram, we can compute the probability distribution for every \\(X_i\\). In the figure above, \\(P_{X_i}(\\texttt{a})\\) is plotted for several values of \\(i\\). The probability to observe an \\(\\texttt{a}\\) seems to stabilize. This leads us to the following definition:")]),a("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[a("h4",{staticStyle:{color:"#bc0031"}},[a("strong",[t._v("Definition: Stationary distribution")])]),t._v("\nA stationary distribution for a time-invariant Markov chain is a distribution \\(P_{X_n}\\) such that \\(P_{X_{n+1}} = P_{X_n}\\).")]),a("p",[t._v("If the initial distribution of a time-invariant Markov process is stationary, then the entire process is stationary as "),a("a",{attrs:{title:"Stationary Process",href:"https://canvas.uva.nl/courses/2205/pages/stationary-process","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/pages/stationary-process","data-api-returntype":"Page"}},[t._v("defined previously")]),t._v(".")]),a("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[a("h4",{staticStyle:{color:"#bc0031"}},[a("strong",[t._v("Proposition")])]),t._v("\nEvery time-invariant finite-state Markov process has a stationary distribution.\n"),a("p",[a("span",{staticClass:"element_toggler",attrs:{role:"button","aria-controls":"group12","aria-label":"Toggler","aria-expanded":"false"}},[a("span",{staticClass:"Button"},[t._v("Proof")])])]),a("div",{attrs:{id:"group12"}},[a("div",{staticClass:"content-box"},[t._v("Let \\(k:= |\\mathcal{X}|\\). The \\(k \\times k\\) transition matrix \\(R\\) with entries \\(R_{ij} = P_{X_{n+1}|X_n}(j|i)\\) is a "),a("a",{attrs:{href:"https://en.wikipedia.org/wiki/Stochastic_matrix"}},[t._v("stochastic matrix")]),t._v(", as for every row \\(i\\), the sum over columns is \\(\\sum_{j=1}^k R_{ij} = 1\\). We are interested in finding a vector \\(v \\in \\mathbb{R}_{\\geq 0}^k\\) such that \\(\\|v\\| = 1\\) and \\(R^Tv = v\\). This vector then represents the stationary distribution. Clearly, a possible eigenvector for \\(R\\) is the all-1 vector \\(w=(1,\\ldots,1)^T\\) because \\(R w = w\\) by definition of a stochastic matrix. Hence, 1 is an eigenvalue of \\(R\\). As \\(R\\) and \\(R^T\\) "),a("a",{attrs:{href:"https://math.stackexchange.com/questions/123923/a-matrix-and-its-transpose-have-the-same-set-of-eigenvalues/123927"}},[t._v("have the same eigenvalues")]),t._v(", 1 is also an eigenvalue of \\(R^T\\); let \\(v \\in \\mathbb{R}^k\\) be the corresponding eigenvector such that \\(R^T v = v\\). If all coordinates of \\(v\\) are non-negative, one can verify that we have found a stationary distribution by renormalizing \\(v / \\sum_{i=1}^k v_i\\). Otherwise, let us write \\(v = v^+ - v^-\\) with \\(v^+,v^- \\in \\mathbb{R}_{\\geq 0}^k\\), where we put all positive coordinates of \\( v \\) in \\(v^+\\) and all negative coordinates of \\( v \\) in \\(v^-\\).Â  Note that \\(R^T v^+ - R^T v^- = R^T (v^+ - v^-) = R^T v = v = v^+ - v^-\\). As all entries of \\(R^T, v^+\\) and \\(v^-\\) are positive, equality must hold for both the positive and negative parts: \\(R^T v^+ = v^+\\) and \\(R^T v^- = v^-\\). As either \\(v^+ \\neq 0^k\\) or \\(v^- \\neq 0^k\\) (otherwise \\(v = 0^k\\), which cannot be the case for an eigenvector), renormalizing that non-zero vector as above yields the stationary distribution.")])])]),a("p",[t._v("Given the transition matrix \\(R\\) of a finite-state Markov process, one can find the stationary distribution \\( \\mu \\) by solving the linear equation \\( \\mu R = \\mu \\) under the constraint that \\( \\sum_i \\mu_i = 1\\).")]),a("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[a("h4",{staticStyle:{color:"#2d3b45"},attrs:{id:"example2"}},[a("strong",[t._v("Example 2: A finite-state time-invariant Markov process, continued")])]),t._v("\nThe matrix representation of the process above is given by \\[ R = \\left[ \\begin{array}{c c} 0.7&0.3\\\\ 0.5&0.5 \\end{array} \\right] \\] . Writing out \\( (\\mu_a, \\mu_b) R = (\\mu_a, \\mu_b) \\) results in \\[ \\left\\{ \\begin{array}{l} 0.7 \\mu_a + 0.5 \\mu_b = \\mu_a \\\\ 0.3 \\mu_a + 0.5 \\mu_b = \\mu_b \\end{array} \\right. \\]. These are linearly dependent equations, but together with the constraint \\( \\mu_a + \\mu_b = 1 \\), they can be solved to \\( (\\mu_a , \\mu_b) = (5/8, 3/8) \\).")])])}],!1,null,null,null);s.options.__file="README.md";e.default=s.exports}}]);