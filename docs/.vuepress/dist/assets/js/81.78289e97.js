(window.webpackJsonp=window.webpackJsonp||[]).push([[81],{49:function(t,e,a){"use strict";a.r(e);var o=a(0),n=Object(o.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"fano-s-inequality"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fano-s-inequality","aria-hidden":"true"}},[t._v("#")]),t._v(" Fano's Inequality")]),a("p",[t._v("Suppose you see \\(Y\\), the output of some noisy channel, and you want to guess what the input to the channel must have been. Let your guess \\(\\hat{X}\\) be some function of your observation of \\(Y\\), that is, \\(\\hat{X} = g(Y)\\). Note that \\(X \\to Y \\to \\hat{X}\\) forms a Markov chain.")]),a("p",[t._v("Fano's inequality relates the probability that your guess is wrong \\(P[\\hat{X} \\neq X])\\) to \\(H(X|Y)\\): the uncertainty you have about the channel's input \\(X\\) when you are only given the output \\(Y\\).")]),a("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[a("h4",{staticStyle:{color:"#bc0031"}},[a("strong",[t._v("Theorem: Fano's Inequality")])]),t._v("\nLet \\( P_{XY} \\) an arbitrary joint distribution of random variables \\(X\\) and \\(Y \\), and let \\(\\hat{X} = g(Y)\\) for some function \\(g\\). Furthermore, define \\(p_e := P[\\hat{X} \\neq X]\\) to be the probability of error. Then \\[ H(X|Y) \\leq p_e \\cdot \\log(|\\mathcal{X}| - 1) + h(p_e). \\] Since we know that \\(0 \\leq p_e \\leq 1\\), and thus \\(h(p_e) \\leq 1\\) we may rewrite Fano's inequality as \\[ p_e \\geq \\frac{H(X|Y) - 1)}{\\log(|\\mathcal{X}|-1)}. \\]\n"),a("p",[a("span",{staticClass:"element_toggler",attrs:{role:"button","aria-controls":"group7","aria-label":"Toggler","aria-expanded":"false"}},[a("span",{staticClass:"Button"},[t._v("Proof")])])]),a("div",{attrs:{id:"group7"}},[a("div",{staticClass:"content-box"},[a("p",[t._v("Define the random variable \\(E\\) to be 0 whenever \\(\\hat{X} = X\\), and 1 otherwise. (In other words, \\(E\\) indicates whether an error has occurred in guessing the input.)")]),a("p",[t._v("Observe the following relations between \\(E\\), \\(X\\), and \\(\\hat{X}\\):")]),a("ol",[a("li",[t._v("\\(H(E|X\\hat{X}) = 0\\) (since \\(E\\) is a function of \\(X\\) and \\(\\hat{X}\\)).")]),a("li",[t._v("\\(H(E|\\hat{X}) \\leq H(E) = h(p_e)\\) (by general properties of conditional entropy).")]),a("li",[t._v("\\(H(X|\\hat{X},E=0) = 0\\) (if you know that the guess was correct, you can infer the original input from the guess).")]),a("li",[t._v("\\(H(X|\\hat{X},E=1) \\leq \\log(|\\mathcal{X}| -1)\\) (if you know that the guess was incorrect, you only know that the correct input was one of the \\(|\\mathcal{X}|-1\\) other options).")])]),a("p",[t._v("These observations allow us to derive the inequality: \\begin{align} H(X|Y) &\\leq H(X|\\hat{X}) &\\text{(by the data-processing inequality)}\\\\ &= H(E|\\hat{X}) + H(X|E\\hat{X}) &\\text{(by entropy diagrams and observation (1))}\\\\ &\\leq h(p_e) + H(X|E\\hat{X}) &\\text{(by observation (2))}\\\\ &= h(p_e) + P_E(0)\\cdot H(X|\\hat{X},E=0) + P_E(1) \\cdot H(X|\\hat{X},E=1)\\\\ &= h(p_e) + 0 + P_E(1) \\cdot H(X|\\hat{X},E=1) &\\text{(by observation (3))}\\\\ &\\leq h(p_e) + p_e \\cdot \\log(|\\mathcal{X}|-1) &\\text{(by observation (4))}.\\\\ \\end{align}")])])])])])}],!1,null,null,null);n.options.__file="README.md";e.default=n.exports}}]);