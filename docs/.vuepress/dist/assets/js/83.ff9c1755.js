(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{47:function(e,t,n){"use strict";n.r(t);var a=n(0),i=Object(a.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"content"},[n("h1",{attrs:{id:"noisy-channel-theorem-converse"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#noisy-channel-theorem-converse","aria-hidden":"true"}},[e._v("#")]),e._v(" Noisy-Channel Theorem: Converse")]),n("p",[e._v("In the previous section, we showed that any rate strictly below the channel capacity is achievable. Here, we show that one cannot do better: rates strictly above the channel capacity are not achievable. Specifically, codes with such rates suffer from non-negligible error probabilities.")]),n("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[n("h4",{staticStyle:{color:"#bc0031"}},[n("strong",[e._v("Theorem: Shannon's noisy-channel coding theorem (converse)")])]),e._v("\nOn a discrete memoryless channel with capacity \\(C\\), any code with rate \\(R > C\\) has average probability of error \\(p_e^{(n)} \\geq 1 - \\frac{C}{R} - \\frac{1}{nR}\\).\n"),n("p",[n("span",{staticClass:"element_toggler",attrs:{role:"button","aria-controls":"group7","aria-label":"Toggler","aria-expanded":"false"}},[n("span",{staticClass:"Button"},[e._v("Proof")])])]),n("div",{attrs:{id:"group7"}},[n("div",{staticClass:"content-box"},[n("p",[e._v("For a code with rate \\(R > C\\), let \\(W\\) be uniformly distributed over all possible messages, let \\(X^n\\) describe the encoding of the message (and the input to the channel), let \\(Y^n\\) describe the output of the channel, and \\(\\hat{W}\\) the decoding of that output.")]),n("p",[e._v("The average probability of error, \\(p_e^{(n)}\\), is equal to \\(P[W \\neq \\hat{W}]\\), the probability that the original message differs from the decoded message. Note that \\(W \\to X^n \\to Y^n \\to \\hat{W}\\) forms a Markov chain.")]),n("p",[e._v("As a first step, we show that the mutual information between the message \\(W\\) and the channel output \\(Y^n\\) is upper bounded by \\(n\\cdot C\\), that is, there is a limit to the amount of information that can get through the channel. To see this, first observe that \\begin{align} H(Y^nW) &= H(Y^nW) + H(X^n \\mid Y^nW) &\\text{(since }W \\text{ determines } X^n\\text{)} \\\\&= H(X^nY^nW) &\\text{(chain rule)} \\\\&= H(X^{n-1}Y^{n-1}W) + H(Y_n \\mid X^nY^{n-1}W) + H(X_n \\mid X^{n-1}Y^{n-1}W) &\\text{(chain rule)} \\\\&= H(X^{n-1}Y^{n-1}W) + H(Y_n \\mid X^nY^{n-1}W)&\\text{(since }W \\text{ determines } X^n\\text{)}\\\\ &= H(X^{n-1}Y^{n-1}W) + H(Y_n \\mid X_n)&\\text{(memoryless)}\\\\ &= \\dots &\\text{(repeat)}\\\\ &= H(W) + \\sum_{i=1}^n H(Y_i \\mid X_i). \\end{align} Therefore, \\begin{align} I(W;Y^n) &= H(W) + H(Y^n) - H(Y^nW) &\\text{(entropy diagram)} \\\\&= H(Y^n) - \\sum_{i=1}^n H(Y_i \\mid X_i) &\\text{(by the above derivation)} \\\\&\\leq \\sum_{i=1}^n H(Y_i) - \\sum_{i=1}^n H(Y_i \\mid X_i) \\\\&= \\sum_{i=1}^n I(X_i;Y_i) \\\\&\\leq n \\cdot C. \\end{align} Now that we have established that \\(I(W;Y^n)\\) is upper bounded by \\(n \\cdot C\\), we can show that the code with rate \\(R\\) induces a considerable error probability: \\begin{align} R &= \\frac{\\log |\\mathcal{W}|}{n} \\\\&= \\frac{1}{n} H(W) \\\\&= \\frac{1}{n} \\left( H(W \\mid Y^n) + I(W;Y^n)\\right) \\\\&\\leq \\frac{1}{n} \\left( H(W \\mid Y^n) + n \\cdot C\\right) \\\\&\\leq \\frac{1}{n} \\left( 1 + P[W \\neq \\hat{W}] \\cdot n \\log |\\mathcal{W}| + n \\cdot C\\right) \\\\&= \\frac{1}{n} + P[W \\neq \\hat{W}] \\cdot R + C, \\end{align} where the second inequality is an application of "),n("a",{attrs:{title:"Fano's Inequality",href:"https://canvas.uva.nl/courses/2205/pages/fanos-inequality","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/pages/fanos-inequality","data-api-returntype":"Page"}},[e._v("Fano's inequality")]),e._v(". Dividing both sides by \\(R\\) and rearranging, we get the desired inequality: \\begin{align} p_e^{(n)} = P[W \\neq \\hat{W}] \\geq 1 - \\frac{C}{R} - \\frac{1}{nR}. \\end{align}")])])])]),n("p",[e._v("This theorem shows that if Alice and Bob try to communicate using a code with a rate \\(R > C\\), their probability of error will be bounded away from zero by a constant factor of \\(1 - \\frac{C}{R}\\) (for big \\(n\\), the last term in the inequality becomes insignificant). This error probability worsens for a bigger difference between \\(R\\) and \\(C\\).")])])}],!1,null,null,null);i.options.__file="README.md";t.default=i.exports}}]);