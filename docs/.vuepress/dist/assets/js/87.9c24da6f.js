(window.webpackJsonp=window.webpackJsonp||[]).push([[87],{43:function(e,t,o){"use strict";o.r(t);var n=o(0),a=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var e=this,t=e.$createElement,o=e._self._c||t;return o("div",{staticClass:"content"},[o("h1",{attrs:{id:"source-channel-separation-theorem-forward-direction"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#source-channel-separation-theorem-forward-direction","aria-hidden":"true"}},[e._v("#")]),e._v(" Source-Channel Separation Theorem: Forward Direction")]),o("p",[e._v("So far in this course, we have treated the 'encoding' of information from two different perspectives:")]),o("ul",[o("li",[e._v("Codes for compressing information (in order to achieve "),o("strong",[e._v("efficient")]),e._v(" communication). "),o("a",{attrs:{title:"Theorem: Shannon's Source-Coding Theorem (Optimal Codes)",href:"https://canvas.uva.nl/courses/2205/pages/theorem-shannons-source-coding-theorem-optimal-codes","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/pages/theorem-shannons-source-coding-theorem-optimal-codes","data-api-returntype":"Page"}},[e._v("Shannon's source-coding theorem")]),e._v(" tells us that the minimal codeword length \\(\\ell_{min}(P_{V^n})\\) for encoding a input block from the source \\(V^n\\) (where all \\(V_i\\) are i.i.d. according to some distribution \\(P_V\\)) is lower bounded by \\(H(V^n) = nH(V)\\). In other words, the rate for such a code, which is \\(R = \\frac{\\log|\\mathcal{V}^n|}{n}\\), is lower bounded by \\(H(V)\\).")]),o("li",[e._v("Codes for protecting information (in order to achieve "),o("strong",[e._v("low-error")]),e._v(" communication). "),o("a",{attrs:{title:"Source-Channel Theorem: Converse",href:"https://canvas.uva.nl/courses/2205/pages/source-channel-theorem-converse","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/pages/source-channel-theorem-converse","data-api-returntype":"Page"}},[e._v("Shannon's channel-coding theorem")]),e._v(" tells us that in order to achieve an arbitrarily low error on the communication over a specific channel, it is necessary that the rate \\(R\\) is strictly upper bounded by the channel capacity \\(C\\).")])]),o("p",[e._v("Combining these two perspectives, we can ask ourselves what happens in the 'sweet spot' where both \\(H(V) \\leq R\\) (as given by the first perspective) and \\(R < C\\) (as given by the second perspective). Do codes with such \\(R\\) always exist? The answer turns out to be yes:")]),o("div",{staticClass:"content-box pad-box-mini border border-trbl border-round"},[o("h4",{staticStyle:{color:"#bc0031"}},[o("strong",[e._v("Theorem: Source-channel separation theorem (forward direction)")])]),e._v("\nLet \\(V_1, V_2, ..., V_n\\) be i.i.d. random variables (the source) distributed according to some \\(P_V\\). Let \\((\\mathcal{X}, P_{Y|X},\\mathcal{Y})\\) be a channel with capacity \\(C\\). If \\(H(V) < C\\), then there exists a "),o("a",{attrs:{title:"A source-channel code encodes blocks of source symbols, V^n, into blocks of inputs for the channel, X^n.","data-tooltip":'{"tooltipClass":"popover popover-padded", "position":"right"}'}},[e._v("source-channel code")]),e._v(" with error probability \\(P[\\hat{V}^n \\neq V^n] \\to 0\\) as \\(n \\to \\infty\\).\n"),o("p",[o("span",{staticClass:"element_toggler",attrs:{role:"button","aria-controls":"group7","aria-label":"Toggler","aria-expanded":"false"}},[o("span",{staticClass:"Button"},[e._v("Proof")])])]),o("div",{attrs:{id:"group7"}},[o("div",{staticClass:"content-box"},[o("p",[e._v("The main idea is to construct our code in two steps: first, we optimally compress the source. Then, we apply an error-correcting code to that compression. Let \\(0 < \\varepsilon < C-H(V)\\). We will exhibit a code that has error probability at most \\(\\varepsilon\\). (This immediately implies the existence of such a code for \\(\\varepsilon \\geq C-H(V)\\) as well.) For the first part (compression), consider the typical set \\(A^{(n)}_{\\varepsilon/2}\\) for the source \\(P_V\\). The typical set has size at most \\(2^{n(H(V) + \\frac{\\varepsilon}{2})}\\), and, for large enough \\(n\\), contains at least \\(1-\\frac{\\varepsilon}{2}\\) of the probability mass of the source. Hence, we need \\(n(H(V)+\\frac{\\varepsilon}{2})\\) bits to compress the source, with a loss (error) of at most \\(\\frac{\\varepsilon}{2}\\).")]),e._v("\nFor the second part (error-correction), we just learned that it is possible (for large enough \\(n\\)) to transmit with error less than \\(\\frac{\\varepsilon}{2}\\), as long as \\(R < C\\). Since we have compressed \\(n\\) source symbols into \\(n(H(V) + \\frac{\\varepsilon}{2})\\) bits, the rate we want to achieve is \\(R = H(V) + \\frac{\\varepsilon}{2}\\). By assumption that \\(\\varepsilon < C - H(V)\\), it follows that \\(R < C\\), such a code indeed exists. Combining the two codes, each with error at most \\(\\frac{\\varepsilon}{2}\\), and applying the union bound, we get \\begin{align} P[\\hat{V}^n \\neq V^n] &\\leq P[V^n \\not\\in A^{(n)}_{\\varepsilon/2}] + P[\\hat{V}^n \\neq V^n | V^n \\in A^{(n)}_{\\varepsilon/2}]\\\\ &\\leq \\frac{\\varepsilon}{2} + \\frac{\\varepsilon}{2}\\\\ &= \\varepsilon. \\end{align}")])])]),o("p",[e._v("It is interesting to note that the i.i.d. assumption on the source can be relaxed, and the theorem actually holds for any finite-alphabet stochastic process satisfying the AEP and entropy rate \\( H(\\{V_i\\}) < C \\).")]),o("p")])}],!1,null,null,null);a.options.__file="README.md";t.default=a.exports}}]);