(window.webpackJsonp=window.webpackJsonp||[]).push([[90],{40:function(e,o,t){"use strict";t.r(o);var i=t(0),n=Object(i.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var e=this,o=e.$createElement,t=e._self._c||o;return t("div",{staticClass:"content"},[t("h1",{attrs:{id:"course-content-overview-now-with-video"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#course-content-overview-now-with-video","aria-hidden":"true"}},[e._v("#")]),e._v(" Course content (overview, now with video!)")]),t("p",[t("iframe",{attrs:{src:"https://player.vimeo.com/video/296892943?color=ff9933",width:"640",height:"360",allowfullscreen:"allowfullscreen",webkitallowfullscreen:"webkitallowfullscreen",mozallowfullscreen:"mozallowfullscreen"}})]),t("p",[e._v("(Download the "),t("a",{staticClass:"instructure_file_link instructure_scribd_file",attrs:{title:"IT Content Intro.pdf",href:"https://canvas.uva.nl/courses/2205/files/476888/download?verifier=39jFWrVkAoqFxywKMEyTpX05EebOX3HGKYqQGqWc&wrap=1","data-api-endpoint":"https://canvas.uva.nl/api/v1/courses/2205/files/476888","data-api-returntype":"File"}},[e._v("slides from the video")]),e._v(' to "click on the links")')]),t("p",[e._v("For a detailed overview of course content and learning objectives, see "),t("a",{attrs:{href:"http://studiegids.uva.nl/xmlpages/page/2018-2019-en/search-course/course/62683"}},[e._v("Studiegids")]),e._v(".Â ")]),t("p",[e._v("The content of this course is organized into seven modules (one for each week the course runs, except the last week in which you will make a final exam):")]),t("h4",[e._v("01 Probability Theory")]),t("p",[e._v("This is an introductory module that you will prepare before the start of the first lecture. It allows you to check your knowledge about logarithms, probability theory, and programming.")]),t("h4",[e._v("02 Entropy")]),t("p",[e._v("This module introduces some of the core concepts of information theory: entropy, conditional entropy, and mutual information. You will learn how to work with these concepts, and how to relate them to each other.")]),t("h4",[e._v("03 Source Coding / Data Compression")]),t("p",[e._v("In this module, you will learn how to compress sources of data that are not uniformly random (e.g., files of English text). You will see some specific algorithms (in pseudo-code) and learn why they perform optimally.")]),t("h4",[e._v("04 Typical Sets and Encryption")]),t("p",[e._v("The first half of this module continues with the topic of data compression, and introduces the concept of typical sets to aid in efficient (but lossy) compression. In the second half, you will learn how to perform perfectly (information-theoretically) secure encryption, and what that necessarily costs.")]),t("h4",[e._v("05 Random Processes")]),t("p",[e._v('In this module, you will study various random processes (such as Markov Chains), and learn how to measure their randomness using the concept of "entropy rate". We will also consider random walks on graphs in this module.')]),t("h4",[e._v("06 Error Correction and Zero-Error Transmission")]),t("p",[e._v("This module considers the problem of trying to send a message over a noisy channel, and limiting the probability that the message (information) is lost while doing so. You will explore how much information you can send over different types of channels if you want to eliminate the probability of loss completely.")]),t("h4",[e._v("07 Noisy Channel Coding")]),t("p",[e._v("This model continues with the topic of module 06, but takes a more fundamental approach. We will finish by proving Shannon's famous source-channel coding theorem, which determines how much information can be sent over a noisy channel if some (arbitrarily small) probability of losing the message is allowed.")])])}],!1,null,null,null);n.options.__file="README.md";o.default=n.exports}}]);