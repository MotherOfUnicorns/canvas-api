<p>We end the theoretic preliminaries with a (non-exhaustive) list of common probability distributions that you will come across throughout the course.</p>
<ul>
<li>The distribution of a biased coin with probability \(P_X(1)=p\) to land heads, and a probability of \(P_X(0)=1-p\) to land tails is called <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution"><span style="color: #bc0031;"><strong>Bernoulli(\(p\))</strong></span> distribution</a>. The expected value is \(\mathbb{E}[X]=p\) and the variance is \(\text{Var}[X]=p(1-p)\).</li>
<li>When \(n\) coins \(X_1, X_2, \ldots, X_n\) are flipped independently and every \(X_i\) is Bernoulli(\(p\)) distributed, let \(S=\sum_{i=1}^n X_i\) be their sum, i.e., the number of heads in \(n\) throws of a biased coin. Then, \(S\) has the <a href="https://en.wikipedia.org/wiki/Binomial_distribution"><span style="color: #bc0031;"><strong>binomial(\(n,p\))</strong></span> distribution</a>: \[ P_S(k) = \binom{n}{k} p^k (1-p)^{n-k} \ \ \ \ \ \text{ where } k=0,1,2,...,n \, . \]<br>From simple properties of the expected value and variance, one can show that \(\mathbb{E}[S]=np\) and \(\text{Var}[S]=np(1-p)\).</li>
<li>The <a href="https://en.wikipedia.org/wiki/Geometric_distribution"><span style="color: #bc0031;"><strong>geometric(\(p\))</strong></span> distribution</a> of a random variable \(Y\) is defined as the number of times one has to flip a Bernoulli(\(p\)) coin before it lands heads: \[ P_Y(k) = (1-p)^{k-1} p \ \ \ \ \ \text{ where } k=1,2,3,... \, .\]<br>There is another variant of the geometric distribution used in the literature, where one excludes the final success event of landing heads in the counting: \[ P_Z(k) = (1-p)^k p \, \ \ \ \ \ \mbox{ where } k=0,1,2,3, ... \, . \]<br>While the expected values are slightly different, namely \(\mathbb{E}[Y]=\frac{1}{p}\) and \(\mathbb{E}[Z]=\frac{1-p}{p}\), their variances are the same: \(\text{Var}[Y]=\text{Var}[Z]=\frac{1-p}{p^2}\).</li>
</ul>