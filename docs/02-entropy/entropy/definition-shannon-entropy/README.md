# Definition: Shannon Entropy

<p>In this section, we explore a measure for the amount of uncertainty of random variables. Watch the video below for an introduction to this topic:</p>
<p style="text-align: center;"><video preload="none" class="instructure_inline_media_comment" data-media_comment_id="m-u7oeQeWyo97fFFYmUAQiWVVyUPq3KDg" data-media_comment_type="video" controls="controls" poster="https://canvas.uva.nl/media_objects/m-u7oeQeWyo97fFFYmUAQiWVVyUPq3KDg/thumbnail?height=448&amp;type=3&amp;width=550" src="https://canvas.uva.nl/courses/10933/media_download?entryId=m-u7oeQeWyo97fFFYmUAQiWVVyUPq3KDg&amp;media_type=video&amp;redirect=1" data-alt=""></video></p>
<p>Consider some probabilistic event \(\mathcal{A}\) that occurs with probability \(P[\mathcal{A}]\) for some probability measure \(P\). The <span style="color: #bc0031;"><strong>surprisal value</strong></span> \(\log \frac{1}{P[\mathcal{A}]}\) indicates how surprised we should be when the event \(\mathcal{A}\) occurs: events with small probabilities yield high surprisal values, and vice versa. An event that occurs with certainty (\(P_X(\mathcal{A}) = 1\)) yields a surprisal value of 0.<span style="color: #ff00ff;"></span></p>
<p>For a random variable \(X\), we consider the <i>expected</i> surprisal value to be an indicator of how much uncertainty is contained in the variable, or how much information is gained by revealing the outcome. This expected surprisal value is more commonly known as the <a title="Shannon once said: My greatest concern was what to call it. I thought of calling it 'information', but the word was overly used, so I decided to call it 'uncertainty'. When I discussed it with John von Neumann, he had a better idea. Von Neumann told me: ''You should call it 'entropy', for two reasons. In the first place, your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, nobody knows what entropy really is, so in a debate you will always have the advantage.''" data-tooltip='{"tooltipClass":"popover popover-padded", "position":"right"}'>(Shannon) entropy</a> of a random variable:</p>
<div id="defEntropy" class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Entropy</strong></h4>
Let \(X\) be a random variable with image \({\cal X}\). The (Shannon) entropy \(H(X)\) of \(X\) is defined as \[ H(X) := \sum_{x\in {\cal X}} P_X(x) \cdot \log \frac{1}{P_X(x)} = -\sum_{x\in {\cal X}} P_X(x) \cdot \log P_X(x) \, , \] with the convention that the \(\log\) function represents the <i>binary</i> logarithm \(\log_2\).</div>
<p>There are three things to note about this definition:</p>
<ul>
<li>The entropy of \(X\) is a function (solely) of the <i>distribution</i> \(P_X\) of \(X\), and the elements of \(\mathcal{X}\) (i.e., the values that \(X\) can take on) are completely irrelevant for the entropy. Therefore, it would be formally more correct to write \(H(P_X)\). However, it is customary (and more convenient) to write \(H(X)\) whenever there is no ambiguity about the underlying distribution.</li>
<li>The summand \(P_X(x) \cdot \log P_X(x)\) is technically undefined whenever \(P_X(x) = 0\). As a convention, we set \(P_X(x) \cdot \log P_X(x) = 0\) in this case. This choice is justified by taking the limit where \(P_X(x)\) goes to 0 (see the exercise below).</li>
<li>The entropy of \(X\) can also be expressed as the expectation of the random variable \(\log\bigl(1/P_X(X)\bigr)\): \[H(X)= \mathbb{E}_X\Bigl[\log\frac{1}{P_X(X)}\Bigr] \, . \]</li>
</ul>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Exercise</strong></h4>
<p>Prove that \(\lim_{p \rightarrow 0^+} p \log(p) = 0\). This is a justification for the convention that we set \(P_X(x) \cdot \log P_X(x) = 0 \) whenever \(P_X(x) = 0\).</p>
<p><span class="element_toggler" role="button" aria-controls="group3" aria-label="Toggler" aria-expanded="false"><span class="Button">Show solution</span></span></p>
<div id="group3" style="">
<div class="content-box">First, note that \[ \lim_{p \rightarrow 0^+} \log p = -\infty \] and \[ \lim_{p \rightarrow 0^+} \frac{1}{p} = \infty. \] So, by <a href="http://mathworld.wolfram.com/LHospitalsRule.html">l'Hopital's rule</a>, \[ \lim_{p \rightarrow 0^+} p \log p = \lim_{p \rightarrow 0^+} \frac{\log p}{1/p} = \lim_{p \rightarrow 0^+} \frac{[\log p]'}{[1/p]'} = \lim_{p \rightarrow 0^+} -\ln(2)\frac{1/p}{1/p^2} = \lim_{p \rightarrow 0^+} -\ln(2)p = 0. \]</div>
</div>
</div>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Example</strong></h4>
Consider a random variable \(X\) with \(\mathcal{X} = {a,b,c,d}\) and \(P_X(a) = \frac{1}{2}\), \(P_X(b) = P_X(c) = \frac{1}{4}\), \(P_X(d) = 0\). The entropy of \(X\) is \[ H(X) = \frac{1}{2} \log 2 + \frac{1}{4} \log 4 + \frac{1}{4} \log 4 + 0 = \frac{1}{2} + \frac{1}{2} + \frac{1}{2} + 0 = \frac{3}{2}. \]</div>