<p>Visualization using "language model" of controller presses (X,Y,A,B)</p>
<p>Â </p>
<p>In this section, we study a different kind of code that can handle context-dependent distributions, unlike the Huffman code. Binary representations of numbers in the interval \([0,1)\) as studied in the previous exercise give rise to a very elegant code:</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Arithmetic code</strong></h4>
Given a source \(P_X\) with \(\mathcal{X} = \{x_1, .., x_m\}\), construct the arithmetic code as follows. Divide the interval \([0,1)\) into disjoint subintervals \(I_{x_j} = [a_j, a_{j+1})\), where \(a_1, \ldots, a_{m+1}\) are defined such that \(a_{j+1} - a_j = P_X(x_j)\), and \(a_1 = 0, a_{m+1} = 1\). The encoding \(AC(x_j)\) of the element \(x_j\) is the (shortest possible) standard binary representation of some number in the interval \(I_{x_j}\).</div>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Example</strong></h4>
Let \(X\) be a random variable with \(\mathcal{X} = \{1,2,3\}\) and \(P_X(1) = \frac{1}{2}\), \(P_X(2) = P_X(3) = \frac{1}{4}\). The arithmetic code is constructed by first determining the intervals:
<div class="content-box pad-box-mini"><img style="display: block; margin-left: auto; margin-right: auto;" src="/img/181295/download?verifier=FRv17xCeUI1MtnvU3BalQhLHz6kw8kRfuNqAdmUy" alt="minimal-12.svg" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/181295" data-api-returntype="File"></div>
This image results in the arithmetic code \(\mathcal{C}\) with \(\mathcal{C}(1) = 0\) (the representation of 0), \(\mathcal{C}(2) = 1\) (the representation of \(\frac{1}{2}\)), \(\mathcal{C}(2) = 11\) (the representation of \(\frac{3}{4}\)). For \(P_X\), the codewords happen to fall exactly <i>on</i> the boundaries of the intervals. This is not always the case, however. The same code would have resulted from this procedure if we started with the random variable \(Y\) with \(\mathcal{Y} = \{1,2,3\}\) and \(P_Y(1) = P_Y(2) = 0.3\) and \(P_Y(3) = 0.4\):
<div class="content-box pad-box-mini "><img style="display: block; margin-left: auto; margin-right: auto;" src="/img/181296/download?verifier=SvuNm2TDBiT1MZPEfyMZP0VbRnWUBR8y7423Bndk&amp;wrap=1" alt="minimal-13.svg" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/181296" data-api-returntype="File"></div>
</div>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Proposition</strong></h4>
For any \((X,P_X)\), the arithmetic code has average length \(\ell_{AC}(P_X) \leq H(X) + 1\).
<p><span class="element_toggler" role="button" aria-controls="group14" aria-label="Toggler" aria-expanded="false"><span class="Button">Proof</span></span></p>
<div id="group14" style="">
<div class="content-box">Let \(x \in \mathcal{X}\), and define \(\ell_x := \lceil \log (1/P_X(x))\rceil\) to be the rounded surprisal value of \(x\). Then \begin{align} 2^{-\ell_x} = 2^{-\lceil \log (1/P_X(x))\rceil} \leq 2^{-\log (1/P_X(x))} = 2^{\log P_X(x)} = P_X(x). \end{align} Therefore, since the size of the interval \(I_x\) is \(P_X(x)\), there must exist an integer \(0 \leq s_x &lt; 2^{\ell_x}\) such that \(s_x \cdot 2^{-\ell_x}\) lies in the interval \(I_x\). This number \(s_x \cdot 2^{-\ell_x}\) has a binary representation of length \(\ell_x \leq -\log P_X(x) + 1\). Repeating this argument for every \(x\), we obtain \begin{align} \ell_{AC}(P_X) = \mathbb{E}[\ell(AC(X))] = \sum_x P_X(x) \ell(AC(x)) \leq \sum_x P_X(x) (-\log P_X(x) + 1) = H(X) + 1. \end{align}</div>
</div>
</div>
<p>As we can see from the example above, this construction for arithmetic codes does not necessarily yield prefix-free codes. However, at the expense of one extra bit of code (on average), the construction can be adapted into a prefix-free code. One example of this is the <span style="color: #bc0031;"><strong>Shannon-Fano-Elias code</strong></span> (see <a href="http://onlinelibrary.wiley.com/book/10.1002/0471200611" target="_blank">Cover/Thomas</a>, Section 5.9 or <a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano%E2%80%93Elias_coding">Wikipedia</a>), which provides a more sophisticated way of selecting a number within each interval than simply selecting the number with the shortest binary representation. This alternative selection procedure ensures prefix-freeness. Another option is to select <i>binary intervals</i> within each interval:</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Arithmetic code (prefix-free version)</strong></h4>
A prefix-free arithmetic code is identical to the definition above, except that the encoding \(AC^{pf}(x_j)\) of the element \(x_j\) is now the name of a largest binary interval that fits entirely in \(I_{x_j}\).</div>
<p>Similarly to the proposition above, it can be shown that for any source \(P_X\), \(\ell_{AC^{pf}}(P_X) \leq H(X) + 2\). Note that we get prefix-freeness only at the expense of an extra bit on average.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Example</strong></h4>
Let \(Y\) be the random variable as in the example above, that is, \(P_Y(1) = P_Y(2) = 0.3\) and \(P_Y(3) = 0.4\). The prefix-free code for \(Y\) is constructed as follows: <br><img src="/img/519668?verifier=VmBKp14wgj38zJ67pwlPJQmRLREDGDSmJgYm8g6c" alt="ArithmeticCode.png" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/519668" data-api-returntype="File"><br>This results in the codewords \(\mathcal{C}(1) = 00\), \(\mathcal{C}(2) = 011\), and \(\mathcal{C}(3) = 11\).</div>
<p>The arithmetic code is slightly less efficient than the Huffman code in terms of average codeword length. A big advantage is the way it is able to adapt to changing distributions, such as when we are encoding a stream of English text. Suppose we are given the (not necessarily i.i.d.) random variables \(X_1, X_2, \ldots, X_n\), and we want to encode the source \(P_{X_1X_2 \cdots X_n}\). We start by dividing the interval \([0,1)\) into subintervals according to \(P_{X_1}\). If, for example, the event \(X_1 = b\) happens, we zoom into the interval corresponding to \(b\), and subdivide <i>that</i> interval according to \(P_{X_2|X_1}\), so that the sizes of these intervals add up to \(P_{X_1}(b)\).</p>
<p>The concept of arithmetic coding is exploited as an accessibility tool in the keyboard alternative <a href="http://wol.ra.phy.cam.ac.uk/dasher/">Dasher</a>, invented by the group of David MacKay at Cambridge University, UK. Try to download and play with it, it's great fun!</p>
<p><img src="http://www.inference.org.uk/dasher/images/newdasher.gif" alt="Demo of Dasher" width="250" height="230"></p>