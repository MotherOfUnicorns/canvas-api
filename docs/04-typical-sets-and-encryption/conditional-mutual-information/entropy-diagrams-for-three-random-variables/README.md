<p>Just like the <a title="Entropy Diagrams for Two Random Variables" href="https://canvas.uva.nl/courses/2205/pages/entropy-diagrams-for-two-random-variables" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/entropy-diagrams-for-two-random-variables" data-api-returntype="Page">entropy diagrams for two random variables</a>, we can visualize the relations between entropy, conditional entropy, mutual information, and conditional mutual information for three random variables. From the diagram, one can easily read off all the relations between the information-theoretic measures, like for instance \(H(X|YZ) = H(X) -I(X;Z) - I(X;Y|Z)\), which is a relation that is otherwise not immediately obvious.</p>
<p>One subtlety with the entropy diagram for three random variables is that the "area in the middle'', \(R(X;Y;Z) := I(X;Y) - I(X;Y|Z)\), may be <i>negative</i>. All other areas and quantities in the diagram are non-negative.</p>
<p>Â </p>
<p><img src="/img/577750?verifier=K7I1ESSiWZElS53cJ247HhqadwhmC9IdVbEIsiQP" alt="EntropyDiagram3RVs.png" width="400" height="423" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/577750" data-api-returntype="File"></p>