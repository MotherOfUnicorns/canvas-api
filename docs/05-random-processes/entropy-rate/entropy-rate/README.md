<p>Intuitively, some of the stochastic processes we have seen in the previous sections are more predictable than others. The periodic Markov process from <a title="Markov Process: Irreducibility, Periodicity, Convergence" href="https://canvas.uva.nl/courses/2205/pages/markov-process-irreducibility-periodicity-convergence#example3" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/markov-process-irreducibility-periodicity-convergence%23example3" data-api-returntype="Page">Example 3</a> is not so surprising anymore as soon as the first \(\texttt{b}\) is observed. In this section, we will study a measure for the unpredictability of a stochastic process: the entropy rate.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Entropy rate</strong></h4>
The entropy rate \(H(\{X_i\})\) of a stochastic process \(\{X_i\}\) is \[ H(\{X_i\}) := \lim_{n \to \infty} \frac{1}{n} H(X_1, \dots, X_n), \] if the limit exists, and undefined otherwise.</div>
<p>In the literature, the entropy rate is often denoted \(H(\mathcal{X})\), referring to the common support of the variables in the stochastic process. The notation \(H(X)\) is also sometimes used, but this can be ambiguous and confusing. The entropy rate reflects the way in which the entropy of the sequence (observed so far) grows as \(n\) grows large.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Example</strong></h4>
Consider a process \(\{X_i\}\) where the \(X_i\) are i.i.d. sampled from \(P_X\). Then \begin{align*} H(\{X_i\}) &amp;= \lim_{n \to \infty} \frac{1}{n} H(X_1, \dots X_n)\\ &amp;= \lim_{n \to \infty} \frac{1}{n} \left( H(X_1) + H(X_2) + \ldots + H(X_n) \right)\\ &amp;= \lim_{n \to \infty} \frac{n}{n} H(X)\\ &amp;= H(X). \end{align*} So, every new coin toss increases the entropy of the entire observed sequence by \(H(X)\).
</div>
<p>We can also define an alternative measure of the unpredictability of a stochastic process, where we focus not on the amount of entropy in the entire sequence observed so far, but on the amount of entropy present in the current random variable, given the past sequence.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Entropy rate given the past</strong></h4>
The entropy rate given the past \(H'(\{X_i\})\) of a stochastic process \(\{X_i\}\) is \[ H'(\{X_i\}) := \lim_{n \to \infty} H(X_n | X_1, \dots, X_{n-1}), \] if the limit exists, and undefined otherwise.</div>
<p>For all stationary processes, this alternative definition turns out to coincide with the original definition of entropy rate. In order to show this, we need an analytic statement about the convergence of sums.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Theorem: Cesàro mean</strong></h4>
If \(\lim\limits_{n \to \infty} a_n = a\) and \(b_n = \frac{1}{n} \sum_{i=1}^n a_i\), then \(\lim\limits_{n \to \infty} b_n = a\).
<p><span class="element_toggler" role="button" aria-controls="group16" aria-label="Toggler" aria-expanded="false"><span class="Button">Show proof</span></span></p>
<div id="group16" style="display: none;">
<div class="content-box"><a id="media_comment_maybe" class="instructure_file_link instructure_video_link" title="05 Cesàro mean.mp4" href="https://canvas.uva.nl/courses/2205/files/591842/download?verifier=bJBODLVOVbFyfIIBKD8lASYPFGfMpWe6WSY09mSH&amp;wrap=1" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/591842" data-api-returntype="File">05 Cesàro mean.mp4</a></div>
</div>
</div>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Theorem</strong></h4>
For a stationary process \(\{X_i\}\), it holds that \(H(\{X_i\}) = H'(\{X_i\})\) (and both limits exist).
<p><span class="element_toggler" role="button" aria-controls="group17" aria-label="Toggler" aria-expanded="false"><span class="Button">Show proof</span></span></p>
<div id="group17" style="display: none;">
<div class="content-box">We first show that \(H(X_n \mid X_1, \dots, X_{n-1})\) is a non-increasing function of \(n\): \begin{align} H(X_{n}|X_1, \dots, X_{n-1}) &amp;= H(X_{n+1}|X_2, \dots, X_{n}) &amp;\text{(stationary)}\\ &amp;\geq H(X_{n+1}|X_1, X_2, \ldots, X_{n}) &amp;(\href{https://canvas.uva.nl/courses/2205/pages/bounds-on-the-conditional-entropy}{\text{Bounds on the Conditional Entropy}}). \end{align} Combined with the fact that \(H(X_n \mid X_1, \dots, X_{n-1})\) is lower bounded by 0, this implies that the limit \(\lim_{n \to \infty} H(X_n \mid X_1, \dots, X_{n-1})\) must exist. It is \(H'(\{X_i\})\). It remains to show that \(H(\{X_i\}) = H'(\{X_i\})\): \begin{align} H(\{X_i\}) &amp;= \lim_{n \to \infty} \frac{1}{n} H(X_1, \dots, X_n)\\ &amp;= \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n H(X_i \mid X_1, \dots, X_{i-1})\\ &amp;= H'(\{X_i\}). \end{align} The final equality follows from the Cesaro mean.</div>
</div>
</div>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 id="example2" style="color: #2d3b45;"><strong>Example 2: A finite-state time-invariant Markov process, continued</strong></h4>
<p>For a Markov process with transition matrix \[ R = \left[ \begin{array}{c c} 0.7&amp;0.3\\ 0.5&amp;0.5 \end{array} \right] \, ,\] we have <a title="Markov Process: Stationary Distribution" href="https://canvas.uva.nl/courses/2205/pages/markov-process-stationary-distribution#example2" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/markov-process-stationary-distribution%23example2" data-api-returntype="Page">previously computed</a> the stationary distribution to be \( (\mu_a , \mu_b) = (5/8, 3/8) \). Hence, if we start in this stationary distribution, the entropy rate for this time-invariant stationary Markov process is \[ H(\{X_i\}) = H'(\{X_i\}) = \lim_{n \to \infty} H(X_n|X^{n-1}) = H(X_2|X_1) \, , \] where \(P_{X_1}\) is the stationary distribution, i.e. \( P_{X_1}(\texttt{a}) = 5/8, P_{X_1}(\texttt{b})=3/8 \). Therefore, \begin{align} H(\{ X_i \}) = H(X_2|X_1) &amp;= P_{X_1}(\texttt{a}) \cdot H(X_2 | X_1 = \texttt{a}) + P_{X_1}(\texttt{b}) \cdot H(X_2 | X_1 = \texttt{b})\\ &amp;= 5/8 \cdot h(0.3) + 3/8 \cdot h(0.5)\\ &amp;\href{https://www.wolframalpha.com/input/?i=5%2F8*(-0.3*log2(0.3)-0.7*log2(0.7))%2B+3%2F8}{\approx} 0.926 \, . \end{align}</p>
<strong>Note that the entropy rate \( H(\{X_i\} \) is not equal to the entropy of the stationary distribution </strong>(which is \( h(5/8) \href{https://www.wolframalpha.com/input/?i=-5%2F8*log2(5%2F8)-3%2F8*log(3%2F8)}{\approx} 0.792 \) in this case)!</div>