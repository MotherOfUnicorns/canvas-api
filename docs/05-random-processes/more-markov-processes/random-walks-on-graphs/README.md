<p>An important and widely applicable example of a time-invariant Markov process is a random walk on a connected graph G with strictly positive symmetric edge weights \( W_{ij} = W_{ji} \). The random walk is defined as follows: at node \( i \), walk to node \( j \) with probability \( \frac{W_{ij}}{W_i} \) where \( W_i := \sum_j W_{ij} \) is the sum of the weights of all edges involving node \( i \), and \( W := \frac12 \sum_i W_i \) is the total of all edge weights.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Example</strong></h4>
For the following graph<img src="/docs/public/img/603108?verifier=S0DPRimWnSuQET3SX4nuwqpUlNfSh3HBthk4UDUs" alt="RandomWalkGraph.png" width="200" height="144" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/603108" data-api-returntype="File">, we have that \( W_1 = 2, W_2 = 4, W_3 = 3, W_4 = 5 \) and \( 2 \cdot W = \sum_i W_i = 2 \cdot 7 \).</div>
<p>The stationary distribution of this random walk is given by \( \mu_i := \frac{W_i}{2W} \), because indeed, at every node \( i \), we have that the sum of all incoming weight is</p>
<p>\[ \sum_j \mu_j \frac{W_{ij}}{W_j} = \sum_j \frac{W_j}{2W} \frac{W_{ij}}{W_j} = \frac{W_i}{2W} = \mu_i \, .\]</p>
<p>We continue to compute the entropy rate of this random walk. Assuming we start in the stationary distribution, we can compute the entropy rate as follows.</p>
<p>\begin{align} H( \{ X_i \} ) = \sum_i \mu_i H( \ldots \frac{W_{ij}}{W_i} \ldots ) &amp;= - \sum_i \mu_i \sum_j \frac{W_{ij}}{W_i} \log \frac{W_{ij}}{W_i} \\ &amp;= - \sum_{i,j} \frac{ W_i} {2W} \cdot \frac{ W_{ij} }{W_i} \log \left( \frac{W_{ij}}{2W} \cdot \frac{2W}{W_i} \right) \\ &amp;= - \sum_{i,j} \frac{ W_{ij}} {2W} \log \left( \frac{W_{ij}}{2W} \right) + \sum_{i,j} \frac{ W_{ij}} {2W} \log \left( \frac{W_{i}}{2W} \right) \\ &amp;= - \sum_{i,j} \frac{ W_{ij}} {2W} \log \left( \frac{W_{ij}}{2W} \right) + \sum_{i} \frac{ W_{i}} {2W} \log \left( \frac{W_{i}}{2W} \right) \\ &amp;= H( \ldots \frac{W_{ij}}{2W} \ldots) - H( \ldots \frac{W_i}{2W} \ldots) \, \end{align} which is the difference of the entropy of the edge distribution and the entropy of the stationary distribution.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Example, continued</strong></h4>
<p>In the example above, the edge distribution is \( \frac{1}{14} (1,1,1,2,2,1,1,1,2,2) \) and the stationary distribution is \( \frac{1}{14}(2,4,3,5) \), resulting in \( H(\{ X_i \}) = H( \frac{1}{14} (1,1,1,2,2,1,1,1,2,2) ) - H( \frac{1}{14}(2,4,3,5) ) \href{https://www.wolframalpha.com/input/?i=-6+*+1%2F14+*+log2(1%2F14)+-+4+*+2%2F14+*+log2(2%2F14)+-+(+-2%2F14*log2(2%2F14)+-+3%2F14*log2(3%2F14)+-+4%2F14*log2(4%2F14)+-+5%2F14*log2(5%2F14))}{\approx} 1.312\)</p>
</div>