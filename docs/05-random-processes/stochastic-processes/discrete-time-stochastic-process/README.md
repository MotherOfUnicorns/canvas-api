<p>Let us now consider more general sets of random variables than the three-variable Markov chains we saw in the previous section. In the rest of this chapter, we will consider infinite sequences of random variables, that can have varying degrees of independence. We start off with the most general definition of such an infinite process.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Discrete-time stochastic process</strong></h4>
A stochastic process is a sequence \(\{X_i : \Omega \to \mathcal{X}\}\) of random variables indexed by \(i \in \mathbb{N}_+\). The process is characterized by the collection of probability distributions \(P_{X_n|X_1 \cdots X_{n-1}}\) for all \(n \in \mathbb{N}_+\).</div>
<p>Equivalently, we can say that a stochastic process is characterized by the joint probability distributions \(P_{X_1 \cdots X_n}\) for all \(n \in \mathbb{N}\). Note that the random variables all have the same domain (the same sample space \(\Omega\)) and the same codomain \(\mathcal{X}\). Often, this sample space is infinite, as it is in the following example.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Example: Repeatedly tossing a fair coin</strong></h4>
Suppose you toss a fair coin an infinite amount of times, and at every step, you count the number of heads you have seen so far. This experiment can be described as a stochastic process by letting each variable \(X_i\) denote the number of heads observed up until that toss. For example, a sequence of tosses \(\texttt{THHTHT} \cdots\) results in the values \(X_1 = 0\), \(X_2 = 1\), \(X_3 = 2\), \(X_4 = 2\), \(X_5 = 3\), \(X_6 = 3, \dots\) . The stochastic process is characterized by the probability distributions \begin{align*} P_{X_1}(0) = P_{X_1}(1) &amp;= \frac{1}{2} \qquad (P_{X_1}(x_1) = 0 \text{ for all other } x_1 \in \mathbb{N})\\ P_{X_{n+1}|X_1 \cdots X_n}(x_{n+1}|x_1\cdots x_{n}) &amp;= \frac{1}{2} \text{   if    } x_{n+1} = x_{n} \text {    or    } x_{n+1} = x_{n} + 1, \text{ and } 0 \text{ otherwise.} \end{align*} For this experiment, the sample space is the set of all possible outcomes for an infinite sequence of coin tosses, \[ \Omega = \{\texttt{H},\texttt{T}\} \times \{\texttt{H},\texttt{T}\} \times \{\texttt{H},\texttt{T}\} \times \cdots \] In particular, \(|\Omega|\) is uncountable. Properly defining a probability measure for \(\Omega\) would require us to rethink quite a few of our definitions that work only for finite sample spaces. That can be done (by using <a href="https://en.wikipedia.org/wiki/Sigma-algebra"><span style="color: #bc0031;"><strong>\(\sigma\)-algebras</strong></span></a> as event spaces), but that is beyond the scope of this course. For stochastic processes, we can always think of the sample space for \(X_n\) as a finite set (consisting of the first \(n\) samples only). In the current example, the probability distribution of \(X_n\) only depends on the first \(n\) coin tosses.</div>