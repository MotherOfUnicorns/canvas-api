<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Markov process</strong></h4>
A stochastic process is a Markov process (or: Markov chain) if for all \(n \in \mathbb{N}\), \[ X_1 \to X_2 \to \cdots \to X_n. \]</div>
<p>In a Markov process, the value of each step can only depend on the previous value, similarly to the three-variable Markov chains from earlier. The <a title="Discrete-Time Stochastic Process" href="https://canvas.uva.nl/courses/2205/pages/discrete-time-stochastic-process" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/discrete-time-stochastic-process" data-api-returntype="Page">infinite sequence of coin tosses</a> from earlier is an example of a Markov process: you can see this by inspecting the definition of \(P_{X_{n+1}|X_1 \cdots X_{n}}\), and noting that it is indeed independent of the values for \(X_1\) to \(X_{n-1}\). The process in this example even fulfills a stronger property: it is time invariant.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Time-invariant Markov process</strong></h4>
A Markov process is time invariant if for all \(n \in \mathbb{N}\), and for all \(a,b \in \mathcal{X}\), \[ P_{X_{n+1} | X_n}(a|b) = P_{X_2 | X_1}(a|b) \qquad \text{whenever } P_{X_n}(b) &gt; 0 \text{ and } P_{X_1}(b) &gt; 0. \]</div>
<p>Time-invariant Markov processes can be nicely visualized using state diagrams, where the <span style="color: #bc0031;"><strong>states</strong></span> represent the values in \(\mathcal{X}\), and the labels on the arrows represent the conditional probabilities. A time-dependent Markov process could also be visualized in this way, but the labels on the arrows would have to be functions of the time step \(i\). In both cases, one also needs to specify \(P_{X_1}\), the <span style="color: #bc0031;"><strong>initial distribution</strong></span>, in order to completely describe the stochastic process.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 id="example1" style="color: #2d3b45;"><strong>Example 1: Repeatedly tossing a fair coin, continued</strong></h4>
The <a title="Discrete-Time Stochastic Process" href="https://canvas.uva.nl/courses/2205/pages/discrete-time-stochastic-process" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/discrete-time-stochastic-process" data-api-returntype="Page">infinite sequence of coin tosses</a><a title="Stationary Process" href="https://canvas.uva.nl/courses/2205/pages/stationary-process" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/stationary-process" data-api-returntype="Page"></a> is represented by the following state diagram: <img src="/docs/public/img/413530?verifier=6rmP6P3cbrTiMbYnQASGrmy7fOcibgI7iTUb2HVe" alt="Screen Shot 2018-10-10 at 10.54.22.png" width="796" height="218" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/413530" data-api-returntype="File"><br>For example, the arrow from state \(\texttt{2}\) to state \(\texttt{3}\), represents the probability \(P_{X_{n+1}|X_{n}}(3|2) = 0.5\). The initial distribution is \(P_{X_1}(0) = P_{X_1}(1) = 0.5\): the process is equally likely to start out in state \(\texttt{0}\) (if the first toss is a tails) or state \(\texttt{1}\) (if the first toss is a heads).</div>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Finite-state Markov process</strong></h4>
A Markov process is finite-state if \(|\mathcal{X}|\) is finite.</div>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 id="example2" style="color: #2d3b45;"><strong>Example 2: A finite-state time-invariant Markov process</strong></h4>
Consider the following state diagram, for a Markov process with initial distribution \(P_{X_1}(\texttt{a}) = 1\) and \(P_{X_1}(\texttt{b}) = 0\): <img src="/docs/public/img/413714?verifier=6BZK0aBCJvIaHnuRZr1HYMjUvA1mjHgYibde17dy" alt="Screen Shot 2018-10-10 at 11.21.20.png" width="400" height="154" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/413714" data-api-returntype="File"><br>The process starts in state \(\texttt{a}\) with probability one. A possible run of the process would be \(\texttt{aabaaabbabaa}\cdots\).</div>
<p>Note that in the state diagrams, the probabilities of the outgoing arrows add up to 1 for every state. This is necessary for a well-defined Markov process. A time-invariant Markov process can alternatively be represented by its initial distribution combined with a <span style="color: #bc0031;"><strong>transition matrix</strong></span> \(R\), where the entry \(R_{ij}\) represents the transition probability \(P_{X_{n+1}|X_{n}}(j|i)\). For a finite-state process, the transition matrix is finite. In the transition matrix, the row entries have to sum up to 1. When the current state is given by a row vector \( v_n \), the state after one step of the Markov process is given by \( v_{n+1} = v R \). If you (like me) don't like multiplying vectors from the left with matrices, you can also compute \( v_{n+1} = \left( R^T v^T \right)^T \) where \( (\cdot)^T \) denotes <a href="https://en.wikipedia.org/wiki/Transpose">the transposition</a> of matrices and vectors.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Example 2: A finite-state time-invariant Markov process, continued</strong></h4>
The matrix representation of the process above is given by \[ R = \left[ \begin{array}{c c} 0.7&amp;0.3\\ 0.5&amp;0.5 \end{array} \right] \] where the state \(\texttt{a}\) is represented by the first row/column, and the state \(\texttt{b}\) by the second. Verify that the row entries indeed sum up to 1. The initial distribution is still given by \(P_{X_1}(\texttt{a}) = 1\) and \(P_{X_1}(\texttt{b}) = 0\).</div>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #2d3b45;"><strong>Another example: primitive language model</strong></h4>
Observe how this very simple 5-state Markov chain produces samples that resemble English language. This is a first hint of how surprisingly powerful Markov models can be.<br><img src="/docs/public/img/603356?verifier=GBsMstEQgj5Fnp1450IfzAxu4LLI79VdM0cLsBdD" alt="MarkovEnglish.png" width="300" height="316" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/files/603356" data-api-returntype="File"><br>(image by Mathias Madsen)</div>