<p>In the previous section it became apparent that for some channels, two or more channel uses can provide a better rate than a single use of those channels. The maximum rate that can be achieved in this way is captured by the notion of Shannon capacity of the confusability graph of the channel.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Shannon capacity of a graph</strong></h4>
The Shannon capacity of a graph is \[c(G) := \sup_{n \in \mathbb{N}} \frac{\log \alpha(G^{\boxtimes n})}{n}.\]</div>
<p>The Shannon capacity represents the <em>maximum number of bits per channel</em> use that can be perfectly communicated over a channel with confusability graph \(G\). Note that our definition in terms of message bits differs from the definition more commonly used in the literature (and on <a href="https://en.wikipedia.org/wiki/Shannon_capacity_of_a_graph">wikipedia</a>) about zero-error communication where the capacity is defined without the log as \( \sup_{n \in \mathbb{N}}  \sqrt[n]{\alpha(G^{\boxtimes n}) } \), thus measuring the maximum number of actual messages (and not bits) per channel use that can be perfectly communicated.</p>
<p>Intuitively, allowing more channel uses can only increase the rate. This is reflected by the fact that we can replace the supremum with a limit, as captured by the following lemma.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Proposition</strong></h4>
\[ c(G) = \lim_{n \to \infty} \frac{\log \alpha(G^{\boxtimes n})}{n}. \]
<p><span class="element_toggler" role="button" aria-controls="group9" aria-label="Toggler" aria-expanded="false"><span class="Button">Show proof</span></span></p>
<div id="group9" style="">
<div class="content-box">First, note that \(\alpha(G^{\boxtimes (k+\ell)}) \geq \alpha(G^{\boxtimes k})\alpha(G^{\boxtimes \ell})\) (you showed this inequality in the previous quiz). It then follows that \[ \log \alpha(G^{\boxtimes(k+\ell)}) \geq \log \alpha(G^{\boxtimes k}) + \log \alpha(G^{\boxtimes \ell}), \] and so the sequence \((\log \alpha(G^{\boxtimes n}))_{n \in \mathbb{N}}\) is superadditive. Then by <a href="https://en.wikipedia.org/wiki/Superadditivity">Fekete's lemma</a>, \[ \lim_{n \to \infty} \frac{\log \alpha(G^{\boxtimes n})}{n} \] exists and is equal to the supremum.</div>
</div>
</div>
<p>The exact computational complexity of \(c(G)\) is unknown. It is not even known to be a decidable problem. We have seen that \(c(C_5)\) is at least \(\frac{\log 5}{2}\), a fact that has been known since Shannon showed it in 1956, but how can we decide whether the capacity is actually bigger than that number? This question remained open until Lovasz <a href="http://web.cs.elte.hu/~lovasz/scans/theta.pdf">showed</a> in <a href="https://doi.org/10.1109/TIT.1979.1055985">1979</a> that the Shannon capacity of \( C_5\) is exactly \(c(C_5) = \frac{\log 5}{2}\). For the relatively small circle of size 7, \(C_7\), the exact Shannon capacity remains unknown. From the fact that \(c(C_5) = \frac{\log 5}{2}\), we know that the 5-letter noisy typewriter does not benefit from more than two channel uses. In fact, all graphs for which the Shannon capacity is known attain that capacity with one, two or an infinite number of channel uses. It is not known if there is a deeper reason for this observed pattern.</p>
<p>It is quite remarkable that in this rather simple and natural problem of zero-error channel coding, we quickly reach the limit of our understanding of the underlying combinatorial problems.</p>
<p> </p>