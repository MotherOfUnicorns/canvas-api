# Definition: Achievable Rate

<p>As stated, the channel capacity reflects the maximum amount of information that could <i>in principle</i> be sent over a noisy channel per use of that channel. The question remains whether this capacity is <span style="color: #bc0031;"><strong>achievable</strong></span> by an actual code in the following sense:</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Definition: Achievable rate</strong></h4>
For a given channel, a rate \(R\) is achievable if there exists a sequence of \((2^{n \cdot R},n)\) codes (for \(n = 1,2,3,...\)) such that \(\lambda^{(n)} \xrightarrow{n \to \infty} 0\). Here, \(\lambda^{(n)}\) is the <a title="Definitions: Code, Rate, and Error Probability" href="https://canvas.uva.nl/courses/10933/pages/definitions-code-rate-and-error-probability" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/10933/pages/definitions-code-rate-and-error-probability" data-api-returntype="Page">maximum error probability</a> as defined in the previous module.</div>
<p>Note that any \((2^{n \cdot R},n)\) code has rate \(R\), since \(\frac{1}{n} \cdot \log {2^{n \cdot R}} = R\). Thus, a rate \(R\) is achievable if there exists a sequence of rate \(R\) codes such that increasing the number of channel uses \(n\) (often called the block size) reduces the maximum error asymptotically to zero.</p>
<p>This final module will be concerned with proving <span style="color: #bc0031;"><strong>Shannon's noisy-channel coding theorem</strong></span>, which states that any rate \(R\) that is strictly below the capacity \(C\) is achievable, and conversely, that any rate strictly above is not achievable.</p>