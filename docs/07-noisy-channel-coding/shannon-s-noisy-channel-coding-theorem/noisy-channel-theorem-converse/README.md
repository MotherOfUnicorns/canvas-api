# Noisy-Channel Theorem: Converse

<p>In the previous section, we showed that any rate strictly below the channel capacity is achievable. Here, we show that one cannot do better: rates strictly above the channel capacity are not achievable. Specifically, codes with such rates suffer from non-negligible error probabilities.</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Theorem: Shannon's noisy-channel coding theorem (converse)</strong></h4>
On a discrete memoryless channel with capacity \(C\), any code with rate \(R &gt; C\) has average probability of error \(p_e^{(n)} \geq 1 - \frac{C}{R} - \frac{1}{nR}\).
<p><span class="element_toggler" role="button" aria-controls="group7" aria-label="Toggler" aria-expanded="false"><span class="Button">Proof</span></span></p>
<div id="group7" style="">
<div class="content-box">
<p>For a code with rate \(R &gt; C\), let \(W\) be uniformly distributed over all possible messages, let \(X^n\) describe the encoding of the message (and the input to the channel), let \(Y^n\) describe the output of the channel, and \(\hat{W}\) the decoding of that output.</p>
<p>The average probability of error, \(p_e^{(n)}\), is equal to \(P[W \neq \hat{W}]\), the probability that the original message differs from the decoded message. Note that \(W \to X^n \to Y^n \to \hat{W}\) forms a Markov chain.</p>
<p>As a first step, we show that the mutual information between the message \(W\) and the channel output \(Y^n\) is upper bounded by \(n\cdot C\), that is, there is a limit to the amount of information that can get through the channel. To see this, first observe that \begin{align} H(Y^nW) &amp;= H(Y^nW) + H(X^n \mid Y^nW) &amp;\text{(since }W \text{ determines } X^n\text{)} \\&amp;= H(X^nY^nW) &amp;\text{(chain rule)} \\&amp;= H(X^{n-1}Y^{n-1}W) + H(Y_n \mid X^nY^{n-1}W) + H(X_n \mid X^{n-1}Y^{n-1}W) &amp;\text{(chain rule)} \\&amp;= H(X^{n-1}Y^{n-1}W) + H(Y_n \mid X^nY^{n-1}W)&amp;\text{(since }W \text{ determines } X^n\text{)}\\ &amp;= H(X^{n-1}Y^{n-1}W) + H(Y_n \mid X_n)&amp;\text{(memoryless)}\\ &amp;= \dots &amp;\text{(repeat)}\\ &amp;= H(W) + \sum_{i=1}^n H(Y_i \mid X_i). \end{align} Therefore, \begin{align} I(W;Y^n) &amp;= H(W) + H(Y^n) - H(Y^nW) &amp;\text{(entropy diagram)} \\&amp;= H(Y^n) - \sum_{i=1}^n H(Y_i \mid X_i) &amp;\text{(by the above derivation)} \\&amp;\leq \sum_{i=1}^n H(Y_i) - \sum_{i=1}^n H(Y_i \mid X_i) \\&amp;= \sum_{i=1}^n I(X_i;Y_i) \\&amp;\leq n \cdot C. \end{align} Now that we have established that \(I(W;Y^n)\) is upper bounded by \(n \cdot C\), we can show that the code with rate \(R\) induces a considerable error probability: \begin{align} R &amp;= \frac{\log |\mathcal{W}|}{n} \\&amp;= \frac{1}{n} H(W) \\&amp;= \frac{1}{n} \left( H(W \mid Y^n) + I(W;Y^n)\right) \\&amp;\leq \frac{1}{n} \left( H(W \mid Y^n) + n \cdot C\right) \\&amp;\leq \frac{1}{n} \left( 1 + P[W \neq \hat{W}] \cdot n \log |\mathcal{W}| + n \cdot C\right) \\&amp;= \frac{1}{n} + P[W \neq \hat{W}] \cdot R + C, \end{align} where the second inequality is an application of <a title="Fano's Inequality" href="https://canvas.uva.nl/courses/10933/pages/fanos-inequality" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/10933/pages/fanos-inequality" data-api-returntype="Page">Fano's inequality</a>. Dividing both sides by \(R\) and rearranging, we get the desired inequality: \begin{align} p_e^{(n)} = P[W \neq \hat{W}] \geq 1 - \frac{C}{R} - \frac{1}{nR}. \end{align}</p>
</div>
</div>
</div>
<p>This theorem shows that if Alice and Bob try to communicate using a code with a rate \(R &gt; C\), their probability of error will be bounded away from zero by a constant factor of \(1 - \frac{C}{R}\) (for big \(n\), the last term in the inequality becomes insignificant). This error probability worsens for a bigger difference between \(R\) and \(C\).</p>