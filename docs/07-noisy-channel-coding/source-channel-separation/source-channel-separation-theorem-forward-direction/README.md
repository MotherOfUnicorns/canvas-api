<p>So far in this course, we have treated the 'encoding' of information from two different perspectives:</p>
<ul>
<li>Codes for compressing information (in order to achieve <strong>efficient</strong> communication). <a title="Theorem: Shannon's Source-Coding Theorem (Optimal Codes)" href="https://canvas.uva.nl/courses/2205/pages/theorem-shannons-source-coding-theorem-optimal-codes" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/theorem-shannons-source-coding-theorem-optimal-codes" data-api-returntype="Page">Shannon's source-coding theorem</a> tells us that the minimal codeword length \(\ell_{min}(P_{V^n})\) for encoding a input block from the source \(V^n\) (where all \(V_i\) are i.i.d. according to some distribution \(P_V\)) is lower bounded by \(H(V^n) = nH(V)\). In other words, the rate for such a code, which is \(R = \frac{\log|\mathcal{V}^n|}{n}\), is lower bounded by \(H(V)\).</li>
<li>Codes for protecting information (in order to achieve <strong>low-error</strong> communication). <a title="Source-Channel Theorem: Converse" href="https://canvas.uva.nl/courses/2205/pages/source-channel-theorem-converse" data-api-endpoint="https://canvas.uva.nl/api/v1/courses/2205/pages/source-channel-theorem-converse" data-api-returntype="Page">Shannon's channel-coding theorem</a> tells us that in order to achieve an arbitrarily low error on the communication over a specific channel, it is necessary that the rate \(R\) is strictly upper bounded by the channel capacity \(C\).</li>
</ul>
<p>Combining these two perspectives, we can ask ourselves what happens in the 'sweet spot' where both \(H(V) \leq R\) (as given by the first perspective) and \(R &lt; C\) (as given by the second perspective). Do codes with such \(R\) always exist? The answer turns out to be yes:</p>
<div class="content-box pad-box-mini border border-trbl border-round">
<h4 style="color: #bc0031;"><strong>Theorem: Source-channel separation theorem (forward direction)</strong></h4>
Let \(V_1, V_2, ..., V_n\) be i.i.d. random variables (the source) distributed according to some \(P_V\). Let \((\mathcal{X}, P_{Y|X},\mathcal{Y})\) be a channel with capacity \(C\). If \(H(V) &lt; C\), then there exists a <a title="A source-channel code encodes blocks of source symbols, V^n, into blocks of inputs for the channel, X^n." data-tooltip='{"tooltipClass":"popover popover-padded", "position":"right"}'>source-channel code</a> with error probability \(P[\hat{V}^n \neq V^n] \to 0\) as \(n \to \infty\).
<p><span class="element_toggler" role="button" aria-controls="group7" aria-label="Toggler" aria-expanded="false"><span class="Button">Proof</span></span></p>
<div id="group7" style="">
<div class="content-box">
<p>The main idea is to construct our code in two steps: first, we optimally compress the source. Then, we apply an error-correcting code to that compression. Let \(0 &lt; \varepsilon &lt; C-H(V)\). We will exhibit a code that has error probability at most \(\varepsilon\). (This immediately implies the existence of such a code for \(\varepsilon \geq C-H(V)\) as well.) For the first part (compression), consider the typical set \(A^{(n)}_{\varepsilon/2}\) for the source \(P_V\). The typical set has size at most \(2^{n(H(V) + \frac{\varepsilon}{2})}\), and, for large enough \(n\), contains at least \(1-\frac{\varepsilon}{2}\) of the probability mass of the source. Hence, we need \(n(H(V)+\frac{\varepsilon}{2})\) bits to compress the source, with a loss (error) of at most \(\frac{\varepsilon}{2}\).</p>
For the second part (error-correction), we just learned that it is possible (for large enough \(n\)) to transmit with error less than \(\frac{\varepsilon}{2}\), as long as \(R &lt; C\). Since we have compressed \(n\) source symbols into \(n(H(V) + \frac{\varepsilon}{2})\) bits, the rate we want to achieve is \(R = H(V) + \frac{\varepsilon}{2}\). By assumption that \(\varepsilon &lt; C - H(V)\), it follows that \(R &lt; C\), such a code indeed exists. Combining the two codes, each with error at most \(\frac{\varepsilon}{2}\), and applying the union bound, we get \begin{align} P[\hat{V}^n \neq V^n] &amp;\leq P[V^n \not\in A^{(n)}_{\varepsilon/2}] + P[\hat{V}^n \neq V^n | V^n \in A^{(n)}_{\varepsilon/2}]\\ &amp;\leq \frac{\varepsilon}{2} + \frac{\varepsilon}{2}\\ &amp;= \varepsilon. \end{align}</div>
</div>
</div>
<p>It is interesting to note that the i.i.d. assumption on the source can be relaxed, and the theorem actually holds for any finite-alphabet stochastic process satisfying the AEP and entropy rate \( H(\{V_i\}) &lt; C \).</p>
<p>Â </p>